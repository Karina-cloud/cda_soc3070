---
title: "Análisis de Datos Categóricos (SOL3070)"
subtitle: "Clase #4"
author: "<br> Mauricio Bucca<br> Profesor Asistente, Sociología UC"
date: "[github.com/mebucca](https://github.com/mebucca)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default","default-fonts","gentle-r.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
---

class: inverse, center, middle

#Valor Esperado y Varianza de variables discretas

---
## Valor Esperado

El valor esperado de una v.a. es el análogo teórico de un promedio. Los posibles valores de la variable se ponderan por su probabilidad de ocurrencia:

<br>
--

\begin{align}
\mathbb{E}(X) &= \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) \\
&\equiv  \sum_{i} x_{i} \times f_{X}(x_{i})
\end{align}

<br>
--
Es teórico porque esta información la podemos saber *a priori*, sin necesidad de datos. 

---
## Valor Esperado

Por ejemplo, supongamos que $Y$ es una variable que resulta de tirar un dado "justo".  ¿Cuál es el valor esperado de $Y$?

<br>
--

\begin{align}
\mathbb{E}(Y) &= \sum_{i}y_{i} \times \mathbb{P}(Y=y_{i})  \\ \\
     &=  1 \times  \frac{1}{6}+ 2 \times \frac{1}{6} + \dots + 6 \times \frac{1}{6}  \\ \\
     &= 3.5
\end{align}


---
## Valor Esperado, algunas propiedades útiles  

1) El valor esperado de una constante es una constante.

$$\mathbb{E}(c)=c$$

--

2) Si $X$ es una variable aleatoria y $c$ una constante, entonces 

$$\mathbb{E}(c X)= c \mathbb{E}(X)$$
--

3) Si $X$ e $Y$ son variables aleatorias (sin importar si $X \bot Y$ o no), entonces

$$\mathbb{E}(X + Y)=  \mathbb{E}(X) + \mathbb{E}(Y)$$
--

4) Si $X$ es una variable aleatoria y $g(.)$ es una función, entonces

$$\mathbb{E}(g(X))=  g(\mathbb{E}(X))$$

--

5. Si $X$ e $Y$ son variables aleatorias independientes, entonces

$$\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$$
---

## Valor Esperado, algunas propiedades útiles  

Por ejemplo, supongamos que $X_{i}$ es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 100 veces. Es premio es $ $1000$ de base, más el resultado de cada dado $i$ multiplicado por 10.
--

 ¿Cuánto es el premio esperado?

--

$$P = 1000 + \sum^{n=100}_{i=1} X_{i} \times 10$$

Por tanto,
--

$$\mathbb{E}(P) = \mathbb{E}(1000 + \sum^{n=100}_{i=1} X_{i} \times 10)$$

--

$$\mathbb{E}(P) = 1000 + 10 \times \sum^{n=100}_{i=1}\mathbb{E}(X_{i})$$

--

$$\mathbb{E}(P) = 1000 + 10 (3.5 + 3.5 + \dots + 3.5)) = 1000 + 10 \times 100 \times 3.5 = \$4500$$ 

---

## Valor Esperado de variables discretas

###  Bernoulli

Si X es una variable Bernoulli, su valor esperado viene dado por:

\begin{align}
\mathbb{E}(X) = \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) &= \sum_{i} x_{i} \times p^{x_{i}}(1-p)^{1 - x_{i}} \\ 
     &= 1 \times p + 0 \times (1-p) \\ 
     &= p
\end{align}

--

### Binomial

Si X es una variable Binomial, su valor esperado viene dado por:

\begin{align}
\mathbb{E}(X) = np
\end{align}

--
.bold[Pregunta]: ¿Cuántas "Caras" debo esperar si tiro 200 monedas "justas"?

--

.bold[Respuesta]: $np = 200 \times 0.5 = 100$


---
## Varianza 

La varianza de una variable aleatoria es el análogo teórico de la varianza de los datos.
--
 Mide cuánta dispersión existe entonce al centro (la media). Formalmente:
 
$$\mathbb{Var}(X) = \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times f_{X}(x_{i})$$
--

Por ejemplo, supongamos que $Y$ es una variable que resulta de tirar un dado "justo".  ¿Cuál es la varianza de $Y$?

<br>
--

\begin{align}
\mathbb{Var}(Y) &= \sum_{i} \bigg( y_{i} - \mathbb{E}(Y) \bigg)^{2} \times f_{Y}(y_{i})   \\ \\
     &=  (1 - 3.5)^{2} \times  \frac{1}{6} + (2-3.5)^{2} \times \frac{1}{6} + \dots + (6-3.5)^{2} \times \frac{1}{6}  \\ \\
     &= 2.91
\end{align}


---
## Varianza, algunas propiedades útiles  

1) La varianzade una constante es cero.

$$\mathbb{Var}(c)=0$$

--

2) Si $X$ es una variable aleatoria y $c$ una constante, entonces 

$$\mathbb{Var}(c X)= c^{2} \mathbb{Var}(X)$$
--

3) Si $X$ e $Y$ son dos variables aleatorias independientes, entoces

$$\mathbb{Var(X \pm Y)=  \mathbb{Var}(X) + \mathbb{Var}(Y)$$

--

5. Si $X$ e $Y$ son variables aleatorias independientes, entonces

$$\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$$
---

### Varianza de una variable Bernoulli

Si X es una variable Bernoulli, su varianza viene dada por:

\begin{align}
\mathbb{Var} = \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times f_{X}(x_{i}) &= (1 - p)^{2} \times p +  (0 - p)^{2} \times (1-p) \\ 
 &=p^{2} − p^{3} + p − 2p^{2} + p^{3} \\ \\
 &=p(1-p)
\end{align}

--

### Varianza de una variable Binomial

Si X es una variable Binomial, su varianza viene dada por:

\begin{align}
\mathbb{Var}(X) = n \times p(1-p)
\end{align}

<br>
--
.bold[Pregunta]: ¿Cuánta variabilidad debo esperar en el número de "Caras" si tiro 200 monedas "justas"?

--

.bold[Respuesta]: varianza es $n \times p(1-p) = 200 \times 0.5 \times 0.5 = 50$. La desviación estándar es $\sqrt{50} = 7.01$.

---
class: inverse, center, middle

#Estimación
##Maximum Likelihood Estimation (MLE)

---

##Estimación 

.bold[Modelos de probabilidad]: ¿Cuál es la probabilidad del observar los *datos* dado los *parámetros* que conocemos?

E.j. ¿Cuán probable es que obtengamos 9 "Caras" (1) si lanzamos una moneda 'justa' ( $p=0.5$ ) 10 veces? 

```{r}
dbinom(x=9,size=10,prob=0.5)
```

---
##Estimación 

.bold[Modelos estadísticos]:  ¿Cuáles son los valores más plausible de los *parámetros* dado los *datos* que observamos? 


E.j. Supongamos que alguien lanza 100 veces la misma moneda y registra los resultados en una base de datos. Los datos se ven así:  

.pull-left[
```{r, echo=FALSE, fig.height=5, fig.width=6, message=FALSE}
library("tidyverse")
set.seed(481)
data_coins <- data.fram\mathbb{E}(X = rbinom(n=100, size=1, prob=0.8))

data_coins %>% ggplot(aes(x=factor(X), fill="")) + 
    geom_bar() +
    geom_text(aes(label=..count..), stat='count', vjust=-0.2) +
    scale_fill_viridis_d() + 
    guides(fill=FALSE, color=FALSE)

```
]

--

.pull-right[

- Lo vemos en la izquierda son .bold[datos]

- Datos: realización de $n$ variables aleatorias 

- Normalmente *no conocemos* la distribución de las variables

- Datos nos da una pista sobre cuál podría ser esa distribución.

- .bold[Estadística]: aprender de los datos para *estimar* los parámetros los generan. 

]

---
##Estimación via Maximum Likelihood (MLE) 

Previamente lanzamos la misma moneda 100 veces y obtuvimos "Cara" (1) 82 veces.
--
 ¿Qué valor de $p$ es más probable que genere estos datos?

MLE es justamente la formalización de esta pregunta. Pasos:

--

1) Decidir sobre la distribución subyacente que genera los datos. En este caso, podemos asumir que: 

  * Cada lanzamiento $X_{1}, X_{2}, \dots X_{100} \sim \text{Bernoulli}(p)$, donde X's son $iid$ 

--

2)  Escribir una función que cuantifique la plausibilidad de diferentes valores del parámetro. Dicha función se denomina .bold[likelihood function]: 

<br>
  * $\mathcal{L}(p) = \mathbb{P}(\text{ Data : \{1,0,1,1,....0,1\}} | \text{ } p)$

<br>
--

  * $\mathcal{L}(p) = \mathbb{P}(X_{1})\mathbb{P}(X_{2}) \dots \mathbb{P}(X_{100}) = p^{82}(1-p)^{18}$


---
##Estimación via Maximum Likelihood (MLE) 

Podemos inspeccionar visualmente la "likelihood" de diferentes valores $p$.

```{r, echo=FALSE, fig.height=5, fig.width=6, message=FALSE}
plot <- ggplot(data = data.fram\mathbb{E}(p = 0), mapping = aes(x = p, color=""))
binom_distrib <- function(p,n,k) (p^(k))*((1-p)^(n-k))

plot + stat_function(fun = binom_distrib, args = list(n= 100, k= 82)) + 
  xlim(0,1) + labs(title="Likelihood of p", x="p", y=expression(past\mathbb{E}(p^{82}, (1-p)^{18})) ) +
    scale_color_viridis_d() + 
    guides(fill=FALSE, color=FALSE)
```

Intuitivamente: habiendo obtenido 82 caras, $p=0.82$ es el valor más plausible de $p$


---

##Estimación via Maximum Likelihood (MLE) 

3) Encontrar matemáticamente el valor de $p$ que maximiza $\mathcal{L}(p)$.


- $\mathcal{L}(p) = P(X_{1})P(X_{2}) \dots P(X_{n}) = p^{k}(1-p)^{n-k} \quad \text{   donde  } k= \sum X_{i}$

--

- Para facilitar el cálculo tomamos logaritmo natural el ambos lados (.bold[log-likelihood])

  - $\ell\ell(p) = \ln \mathcal{L}(p)  = k \ln(p) + (n - k) \ln(1-p)$ 

--
-  Calcular la derivada de $\ell\ell(p)$ con respecto a $p$: pendiente de la curva en cada valor de $p$.

  - $\ell\ell^{\text{ '}}(p) = \frac{k}{p} -  \frac{n-k}{1-p}$

--

- Encontrar el máximo de la función $\ell\ell(p)$: valor de $p$ en el cual la curva no cambia, es decir. cuando $\ell\ell^{\text{ '}}(p)=0$ 

  - $\frac{k}{p} -  \frac{n-k}{1-p} = 0$
  
--

Después de resolver por $p$ obtenemos:
  
  - $p = \frac{k}{n} = \frac{\sum X_{i}}{n}$


---
##Estimación via Maximum Likelihood (MLE) 

<br>

- El estimador ML de $p$ es ....


- $\hat{p} = \frac{\sum X_{i}}{n}$


- Es decir, el porcentaje de 1's en la muestra!

--

- Intuitivo y elegante.
--
 No siempre tan simple....


---

.bold[Ejemplo via simulacioón en R]

```{r loglik_fun,  include=TRUE, echo=TRUE, warning=FALSE, message=FALSE}

# log-likelihood function
ll <- function(p,n,k) {
  ell = k * log(p) + (n - k)*log(1-p)
  return(ll = ell)
}


# Evaluate the log-likelihood function for some arbitrary values
ll(p=0.1,n=100,k=82); ll(p=0.7,n=100,k=82)


# Evaluate the log-likelihood function for many possible combinatios values of p

parameter_space <- tibbl\mathbb{E}(p=seq(0,1,by=0.01)) %>% 
  rowwis\mathbb{E}() %>% mutat\mathbb{E}(loglik = ll(p,n=100,k=82)) 

# Find the value of p that yield the largest value for the log-likelihood function

parameter_space %>% as.matrix() -> m
m[which.max(m[,2]),]
```

--

Así se ve:

```{r loglik_density,  include=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
# Plot all LL values
parameter_space %>% as.data.fram\mathbb{E}() %>% ggplot(aes(x=p, y=loglik, colour=loglik)) + geom_lin\mathbb{E}(size=1) + geom_point(aes(x=m[which.max(m[,2]),1], m[which.max(m[,2]),2])) +
scale_color_viridis() +
guides(fill=FALSE, color=FALSE) + labs(x="p", y="log-Likelihood") +
annotat\mathbb{E}(geom="text", x=0.82, y=-35, label='bold("0.82")', color="black", parse=TRUE) 

```

---
class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca





---
title: "Análisis de Datos Categóricos (SOL3070)"
subtitle: "Clase #8"
author: "<br> Mauricio Bucca<br> Profesor Asistente, Sociología UC"
date: "[github.com/mebucca](https://github.com/mebucca)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default","default-fonts","gentle-r.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
---

class: inverse, center, middle

#Modelos Lineales Generalizados (GLM)

---
## Más allá del modelo de regresión lineal (LM) 

LM es un marco muy útil y productivo, pero hay situaciones en las que no proporciona una descripción adecuada de los datos. En particular:

<br>

--

- Cuando $Y$ no se distribuye normalmente

--

- Cuando el rango de $Y$ está restringido (por ejemplo, binario, recuento)

--

- Cuando la variación de $Y$ no es independiente de la media de $Y$.

<br>
--

GLM ofrece un marco mucho más general y flexible que incorpora y amplía el LM para abordar estas cuestiones.

---
## Estructura de los modelos lineales generalizados

Un modelo lineal generalizado tiene cuatro componentes:

.pull-left[

- Un _componente aleatorio_

- Un _componente sistemático_ 

- Una _función de enlace_ (link).

- Una _función de varianza_
]

.pull-right[
![nelder](nelder.png)
]

---
### Componente aleatorio

$$\newcommand{\vect}[1]{\boldsymbol{#1}}$$

El componente aleatorio de un GLM identifica la distribución de probabilidad de la variable dependiente

<br>

- Al igual que con LM, los datos que queremos modelar ( $\vect{y}$ ) son una colección de observaciones $y_{1}, \dots, y_{n}$, donde cada observación es la manifestación de una variable aleatoria.

--

-  Estas variables aleatorias  $y_{1}, \dots, y_{n}$ son independientes  entre si y provienen de la misma _familia_ de distribución: .bold[iid]

  - La distribución de los datos nos da una pista sobre la distribución de probabilidad subyacente
  - Muestreo aleatorio garantiza que spuesto de independencia se cumpla


---
### Componente aleatorio


Mientras que la LM asume que la variable dependiente sigue una distribución normal, GLM abarca un conjunto más amplio de distribuciones, .bold[tanto continuas como discretas], siempre y cuando pertenezcan a la clase más general de la [_familia exponencial de distribuciones_](https://en.wikipedia.org/wiki/Exponential_family).  

--

![Some distributions of the exponential family and their relationship](expo_fam.png)

---
## Componente Sistemático

El componente sistemático de un GLM especifica las variables explicativas, es decir, las $x$'s en el lado derecho de la ecuación

<br>

$$\eta_{i} = \beta_{0} + \beta_{1} x_{i1} + \dots + \beta_{k} x_{ik}$$
<br>
--

 - En terminología GLM $\eta$ se denomina _predictor lineal_.
 
 - $\eta$ es lineal "en parámetros" pero puede ser no lineal "en variables" (por ejemplo, interacciones, términos cuadrados, etc.). 

--

 - Las $x$'s son tratadas como fijas, no como variables aleatorias.
 
---
## Función de enlace (link)

En el LM estándar, la media condicional del resultado $\mu_{i}$ está linealmente relacionada con los predictores del modelo.

$$\underbrace{\mathbb{E}(y_{i} \mid  x_{1}, \dots x_{k} )}_{\mu_{i}} = \underbrace{\beta_{0} + \beta_{1} x_{i1} + \dots + \beta_{k} x_{ik}}_{\eta_{i}}$$

--

- GLM permiten una relación más general y flexible entre estos dos términos. 

--

- En un GLM el componente sistemático $\eta_{i}$ debe estar relacionado linealmente (en parámetros) con una función $g(\cdot)$ de $\mu_{i}$. Dicha función se denomina *función de enlace*. Formalmente,

$$g(\mathbb{E}(y_{i} \mid  x_{1}, \dots x_{k})) =  g(\mu_{i}) = \eta_{i}$$
--

Ejemplo, si $g(\cdot) = \ln(\cdot)$, entonces


$$\ln \mathbb{E}(y_{i} \mid  x_{1}, \dots x_{k}) =  \ln \mu_{i} = \eta_{i}$$

---

##Función de enlace (link)

- El inverso de esta expresión es la llamada *función media*, que expresa la media condicional de $y_{i}$ como una función potencialmente no lineal de los predictores:

$$\mathbb{E}(y_{i} \mid  x_{1}, \dots x_{k}) = \mu_{i} = g^{-1}(\eta_{i})$$
--

Ejemplo, si $g(\cdot) = \ln(\cdot)$, entonces

$$\ln \mathbb{E}(y_{i} \mid  x_{1}, \dots x_{k}) =  \ln \mu_{i} = \eta_{i} \quad \text{por tanto}$$
--

$$\mathbb{E}(y_{i} \mid  x_{1}, \dots x_{k}) = \mu_{i} = e^{\eta_{i}}$$
<br>
--

- La _función de enlace_ cumple un objetivo importante: mantener $\mu_{i}$ dentro de su rango natural. 

  - Ejemplo: si $y_{i}$ es estrictamente positivo (ingreso), $\eta_{i} \in (-\infty, \infty+)$ pero $\mu_{i} = e^{\eta_{i}} \in  (0, \infty+)$


---
## Función de enlace (link)

Más allá de este ejemplo, hay una variedad de posibles funciones de enlace:


![Some commonly used link functions](link_fn.png)
- $g(\cdot)$ debe ser "smooth" y monotónica

---
## Función de varianza

La función de varianza describe cómo la varianza $\mathbb{Var}(y_{i})$ depende de la media $\mu_{i}$. Formalmente:

<br>

$$\mathbb{Var}(y_{i}) = \phi V(\mu_{i})$$
donde

- $\phi$ es una constante llamada "parámetro de dispersión"

- $V(\cdot)$ es la  función de varianza, definida como:

$$V(\mu) = \frac{d\mu}{d \eta}$$
<br>
--

- La _función de varianza_ es única cada distribución de la familia exponencial.



---
## Definiendo un GLM

La estructura básica de un GLM se especifica mediante la elección de dos componentes: (1) la distribución de la variable dependiente (componente sistemático) y (2) la función de enlace. 

<br>
\begin{align}
  GLM:
	\begin{cases}
	&y_{i} \sim f(\mu_{i},\sigma_{i}) \\ \\
	& g(\mu_{i}) = \eta_{i}
	\end{cases}
\end{align}

<br>
--

Cualquier combinación de estos componentes definirá un GLM diferente. Algunas de estas combinaciones son especialmente relevantes:


| Distribution         | Canonical Link: $\eta = g(\mu)$ | Link name             | Model name           |
| -----------------    | ------------------              | --------------------- | -------------------- |
| Normal (Gaussian)    | $\eta = \mu$                    | identity              | Standard regression  |
| Poisson              | $\eta = \log(\mu)$              | logarithm             | Poisson regression   |
| Bernoulli / Binomial | $\eta = \log(\mu/(1-\mu))$      | logit                 | Logistic regression  |
| Gamma                | $\eta = 1/\mu)$                 | reciprocal            | Gamma regression     |


---
## LM es un caso particular de GLM

El tipo más simple de GLM es el que tiene un componente aleatorio normal y una función de enlace de identidad, en cuyo caso obtenemos el modelo estándar de regresión lineal. Formalmente:


--

- Componente aleatorio: $y_{1}, \dots y_{n}$ son $n$ variables independientes con distribución $\mathcal{N}(\mu_{i},\sigma_{i})$

--

- Función de enlace "identidad": $g(x) = x$

--

- Componente sistemático: $\mu_{i} = \eta_{i} = \beta_{0} + \beta_{1} x_{i1} + \dots + \beta_{k} x_{ik}$

--

- Función media: $\mu_{i} = g^{-1}(\eta_{i}) = \eta_{i}$

--

- Varianza $\mathbb{Var}(y_{i}) = \phi V(\mu_{i}) = \sigma$, 
--
 con $\phi=\sigma$ y $V(\mu_{i})= \frac{d\mu_{i}}{d\eta_{i}}=1$.

--

Por tanto,


$$y_{i} \sim \mathcal{N}(\mu_{i} = \eta_{i}, \sigma_{i} = \sqrt{\phi})$$

- En nomenclatura GLM $\phi$ (parámetro de dispersión) es un modo más general de expesar el componente constante de la varianza ( $\sigma^2$ en LM)

---
## GLM log-linear: ejemplo de modelo "híbrido" 


Ejemplo:

--

- $y_{1}, \dots y_{n}$ son $n$ variables independientes con distribución $\mathcal{N}(\mu_{i},\sigma_{i})$

--

- La función de enlace es $g(\cdot) = \ln(\cdot)$

--

- $\ln(\mu_{i}) = \eta_{i} = \beta_{0} + \beta_{1} x_{i1} + \dots + \beta_{k} x_{ik}$

--

- $\mu_{i} = e^{\eta_{i}}$

--

Se sigue de lo anterior que:


- $\mathbb{Var}(y_{i}) = \phi \frac{d\mu_{i}}{d\eta_{i}} = \phi e^{\eta_{i}} = \phi \mu_{i}$

--

Obtenemos un _modelo log-lineal_, que es apropiado cuando las predicciones deben ser estrictamente positivas.


$$y_{i} \sim \mathcal{N}(\mu_{i} = e^{\eta_{i}}, \sigma_{i} = \sqrt{\phi \mu_{i}})$$
---
## Enlace vs transformaciones  

Es muy común transformar la variable dependiente para cumplir con los supuesto del LM. 

Ejemplo: variable dependiente transforma a logs 

- $y_{1}, \dots y_{n}$ son $n$ variables que no distribuyen normal

- $\ln(y_{1}), \dots \ln(y_{n})$ son $n$ variables que distribuyen normal


Un LM estándar 


$$\ln(y_{i}) = \overbrace{\beta_{0} + \beta_{1}x_{1i} + \dots \beta_{k}x_{ki}}^{\mathbb{E}(\ln y_{i} \mid x_{1}, \dots, x_{k} )  } + \underbrace{e_{i}}_{\mathcal{N}(0,\sigma)}$$



---
class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca





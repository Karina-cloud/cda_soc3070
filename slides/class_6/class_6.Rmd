---
title: "Análisis de Datos Categóricos (SOL3070)"
subtitle: "Clase #6"
author: "<br> Mauricio Bucca<br> Profesor Asistente, Sociología UC"
date: "[github.com/mebucca](https://github.com/mebucca)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default","default-fonts","gentle-r.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
---

class: inverse, center, middle

#Tablas de Contingencia
## Medidas de asociación

---
##  Asociación en tablas de contingencia 

Las variables de una tabla de contingencia están asociadas si la distribución condicional de las variables es distinta de su distribución marginal. Formalmente, 

<br>

- $f_{Y \mid X}(Y \mid X) \neq f_{Y}(Y)$

y por tanto,

- $f_{X \mid Y}(X \mid Y) \neq f_{X}(X)$


---
##  Asociación en tablas de contingencia 

```{r,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library("tidyverse")
library("Ecdat")
data(Fair)
affairsdata <- Fair %>% as_tibble()

# create a binary variable indicating wether persons has ever had an affair
affairsdata <- affairsdata %>% 
	mutate(everaffair = case_when(nbaffairs == 0 ~ "Never", nbaffairs > 0 ~ "At least once") )

ctable <- affairsdata %>% with(table(sex,everaffair))
```

Continuando con nuestro ejemplo,

.pull-left[
$f(\text{everaffair} \mid \text{sex})$
```{r}
prop.table(ctable,1)
```


$f(\text{everaffair})$
```{r}
prop.table(apply(ctable,2,sum))
```

]

--

.pull-right[
Al parece que los hombres tienen una mayor probabilidad que las mujeres de haber tenido un "affair".

En lo que sigue vamos a usar este ejemplo para estudiar:

- Diferentes formas de cuantificar la asociación (o la ausencia de la misma) entre variables de una tabla de contingencia

- Evaluar si las diferencias observadas son o no más sustanciales de lo se esperaría debido al mero azar.
]

---
### Test de indepencia estadística 



---
### Diferencia de proporciones

- Supongamos que tenemos una tabla de contingencia 2-ways que cruza las variables binarias $X$ (independiente) y $Y$ (dependiente).  Éxito se codifica con valor 1 y el fracaso con el valor cero.

- Para detectar la asociación necesitamos medir diferencias en la distribución de $Y$ condicional en $X$

--
La diferencia de proporciones cuantifica estas diferencias de la siguiente manera:

$$\delta = \mathbb{P}(Y=1 \mid X=1) - \mathbb{P}(Y=1 \mid X=0)$$
--

Noten que $\delta \in [-1,1]$ donde $\delta=0$ indica proporciones iguales. 

--

Volviendo a nuestro ejemplo, $\hat{p}_{H}$, llamemos y a la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ a la proporción de mujeres que han tenido una aventura. La diferencia de proporciones se define simplemente como:

\begin{align}
  \hat{\delta} &= \hat{p}_{H} - \hat{p}_{M} \\ \\
  &= 0.273 - 0.229 \\ \\
  &= 0.044
\end{align}

---
### Diferencia de proporciones

Dos consideraciones importantes:

1) La diferencia de proporciones debe estar adecuadamente definida en términos de una variable dependiente y otra independiente. La razón es que, en general:

$$\mathbb{P}(Y=1 \mid X=1) - \mathbb{P}(Y=1 \mid X=0) \neq  \mathbb{P}(X=1 \mid Y=1) - \mathbb{P}(X=1 \mid Y=0)$$

--

En nuestro ejemplo:

```{r}
prop.table(ctable,2)
```

Si tratamos género como variable independiente y definimos "mujeres" como la categoría de éxito, la diferencia en las proporciones es $\delta = 0.48 - 0.54 = -0.06$. 

---
### Diferencia de proporciones

2) La diferencia de proporciones es una estadística intuitiva y fácil de interpretar, pero por sí sola puede ser engañosa cuando las proporciones son ambas cercanas a cero. Consideremos los dos casos hipotéticos siguientes:

\begin{align}
  \text{Caso 1: } p_{1a}=0.410 \text{ and } p_{1b}=0.401  \\ \\
  \text{aquí: } \delta_{1} = 0.009
\end{align}

--
y

\begin{align}
  \text{Caso 2: } p_{2a}=0.010 \text{ and } p_{2b}=0.001  \\ \\
  \text{aquí: } \delta_{2} = 0.009
\end{align}

--
¿Problemas? En el caso 1 ambas porciones son, según todos los indicios, casi idénticas. En el caso 2, sin embargo, ambas proporciones son similares en términos absolutos, por muy diferentes en términos relativos: $0.010$ es 10 veces mayor $0.001$.


---
### Riesgo Relativo (RR)

En casos como el descrito anteriormente el ratio entre las proporciones es una estadística más relevante. El riesgo relativo se define como:

$$RR = \frac{\mathbb{P}(Y=1 \mid X=1)}{\mathbb{P}(Y=1 \mid X=0)}$$

Notar que $RR \in [0,\infty+]$, donde $RR=1$ indica igualdad de proporciones. 

<br>
--

En nuestro ejemplo, el riesgo relativo estimado es:

\begin{align}
  \hat{RR} &= \frac{\hat{p}_{H}}{\hat{p}_{M}} \\ \\
  &= \frac{0.273}{0.229} = 1.19214
\end{align}

--

La proporción de infidelidad entre los hombres es aproximadamente un 20% mayor que entre las mujeres. 

---
### Riesgo Relativo (RR)

Tres consideraciones importantes:

1) Al igual que la diferencia de proporciones, el riesgo relativo debe definirse adecuadamente en términos de una variable dependiente y otra independiente. En general:

$$\frac{\mathbb{P}(Y=1 | X=1)}{\mathbb{P}(Y=1 | X=0)} \neq  \frac{\mathbb{P}(X=1 | Y=1) }{\mathbb{P}(X=1 | Y=0)}$$

--

En nuestro ejemplo:

```{r}
prop.table(ctable,2)
```

Si tratamos género como variable independiente y definimos "mujeres" como la categoría de éxito, la diferencia en las proporciones es $RR = 0.48/0.54 = 0.89$. 

---
### Riesgo Relativo (RR)

2) El riesgo relativo depende que categoría definimos cómo "éxito". En general, 


$$\frac{\mathbb{P}(Y=1 | X=1)}{\mathbb{P}(Y=1 | X=0)} \neq \frac{\mathbb{P}(Y=0 | X=0)}{\mathbb{P}(Y=0 | X=1)}$$
--

En nuestro ejemplo:

.pull-left[
ratio proporción infidelidad H/M
\begin{align}
  \frac{\hat{p}_{H}}{\hat{p}_{M}} &= \frac{0.273}{0.229} \\ \\
   &= 1.19214
\end{align}
]

.pull-left[
ratio proporción fidelidad M/H
\begin{align}
  \frac{1-\hat{p}_{M}}{1-\hat{p}_{H}} &= \frac{0.771}{0.727} \\ \\
   &= 1.060523
\end{align}
]


---
### Riesgo Relativo (RR)

3)  Si una de las proporciones involucradas en el cálculo es demasiado pequeña, el $RR$ puede tomar valores arbitrariamente grandes o arbitrariamente pequeños. 

<br>

  - Ejemplo: $0.7/0.005 = 140$

--

  - En estos casos la definición de la variable dependiente/independiente, y de la la categoría de éxito afectan         
radicalmente el resultado:

    - invirtiendo la categoría de exito: $0.3/0.9 = 0.3$
    
    - invirtiendo la variable dependiente: $0.009/0.768=0.012$, u otros, dependiente de categoría de éxito
  
--

  - Problema muy común cuando se trabaja con eventos con muy baja prevalencia (ej, suicidio, covid, etc.)

---
### Odds Ratio

<br>

- Odds ratio ( $\theta$ ) es una medida fundamental de asociación. 

- Parámetro de interés en el modelo más importante de datos categóricos: regresión logística.

- $\theta$ está formulada para tablas de 2-por-2, pero también puede calcularse para tablas de mayor dimensión: toda tabla $n$-ways, $I \times J$, puede ser re-escrita como  $(I-1) \times (J-1) \times (n-1)$ tablas de 2-por-2.

---
#### Odds

La Odds Ratio es el ratio de dos "odds", donde las "odds" una variable binaria $Y$ se definen como: 

<br>

\begin{align}
  \text{odds} &= \frac{\mathbb{P}(Y=1)}{1-\mathbb{P}(Y=1)} \\ \\
              &=  \frac{p}{1-p}
\end{align}

<br>
--

Por ejemplo, si $Y$ tiene una probabilidad de éxito $p=0.75$, las odds de éxito son $\text{odds}=\frac{0.75}{0.25} = 3$. 

- Esto significa que el éxito es 3 veces más probable que el fracaso.


---
#### Odds

las Odds son funciones de probabilidades y, por lo tanto, las probabilidades también pueden expresarse en función de las odds. Formalmente:

$$p = \frac{\text{odds}}{1 + \text{odds}}$$

--

Siguiendo con ejemplo anterior, si sabemos que las odds de éxito son igual a 3, entonces la probabilidad ( $p$ ) de éxito es:

.pull-left[
\begin{align}
p &= \frac{3}{1 + 3} \\ \\
  &= 0.75
\end{align}
]

--

.pull-right[

.content-box-grey[
.tiny[.bold[Derivación]:
\begin{align}
  \text{odds} &= \frac{p}{1-p}  \text{  } \\ \\
  \text{odds} &= \frac{1}{\frac{1}{p} - 1}  \\ \\
  \frac{1}{p} &= \frac{1}{\text{odds}} + 1  \\  \\
  \frac{1}{p} &= \frac{1 + \text{odds}}{\text{odds}}  \\ \\
           p  &= \frac{\text{odds}}{1 + \text{odds}}
\end{align}
]
]
]

---
### Odds Ratio

Las .bold[odds] resumen la distribución de una sola variable binaria. Para medir la asociación entre dos de estas variables en una tabla podemos calcular la .bold[odds *ratio*]. 

--

Si $X$ e $Y$ son las variables independiente y dependiente, la distribución condicional $f(Y \mid X)$ se puede resumir con dos .bold[odds]:

\begin{align}
  \text{odds}_{0} &=  \frac{\mathbb{P}(Y=1 | X=0) }{1 - \mathbb{P}(Y=1 | X=0) } \quad \text{y} \\ \\
  \text{odds}_{1} &=  \frac{\mathbb{P}(Y=1 | X=1) }{1 - \mathbb{P}(Y=1 | X=1) } 
\end{align}

--

El .bold[odds *ratio*], por tanto, es:

\begin{align}
  \theta &= \frac{\text{odds}_{1}}{\text{odds}_{0}} \\ \\\
\end{align}


---
### Odds Ratio

Volviendo a nuestro ejemplo,

```{r,echo=FALSE}
prop.table(ctable,1)
```

Si $\hat{p}_{H}$ es la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ es la proporción de mujeres que han tenido una aventura. 


\begin{align}
  \hat{\theta} = \frac{\text{odds}_{H}}{\text{odds}_{M}} &= \\
         &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
         &= \frac{0.273/0.727}{0.229/0.771} \\ \\
         &= \frac{0.38}{0.30} = 1.27
\end{align}

---
### Odds Ratio

Dado que estas proporciones se estiman a partir de los recuentos de la tabla, $\theta$ también puede expresarse de la siguiente manera, denominada .bold[cross-product ratio].

En nuestro ejemplo,

.pull-left[
```{r, echo=FALSE}
ctable
```
]

.pull-right[
\begin{align}
  \hat{\theta} &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
   &= \frac{\frac{n_{21}}{n_{2+}} / \frac{n_{22}}{n_{2+}}}{\frac{n_{11}}{n_{1+}}/ \frac{n_{12}}{n_{1+} }} \\ \\
   &= \frac{n_{21} \times n_{12}}{n_{22} \times n_{11}} \\ \\
\end{align}
]

--

```{r}
Theta = (ctable[2,1]*ctable[1,2])/(ctable[2,2]*ctable[1,1]); Theta
```

---
### Odds Ratio

```{r}
Theta = (ctable[2,1]*ctable[1,2])/(ctable[2,2]*ctable[1,1]); Theta
```

.bold[Interpretación]: las odds de que un hombre tenga affair son 1,27 veces mayores que las de una mujer, es decir, 27% más altas. 

Notice that:

- $\theta \in [0,\infty+]$

--

- $\theta=1$ indica igualdad de odds y, por lo tanto, independencia

--

- $\theta > 1$ indica que el éxito es más probable para el grupo en el numerador (hombres en este caso)

--

- $\theta < 1$ indica que el éxito es más probable para el grupo en el denominador (mujeres en este caso)

--

- Valores lejos de 1, en cualquier dirección, representan una fuerte evidencia contra independencia

---
### Propiedades de la Odds Ratio

1) Invirtiendo el orden de las filas o columnas obtenemos el inverso la odds ratio original.

```{r,echo=FALSE}
prop.table(ctable,1)
```

Si $\hat{p}_{H}$ es la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ es la proporción de mujeres que han tenido una aventura. 

.pull-left[
\begin{align}
  \hat{\theta}_{HM} &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
         &= \frac{0.38}{0.30} \\ \\
         &= 1.27
\end{align}
]

.pull-right[
\begin{align}
  \hat{\theta}_{HM} &= \frac{\hat{p}_{M}/(1 - \hat{p}_{M})}{\hat{p}_{H}/(1 - \hat{p}_{H})} \\ \\
         &= \frac{0.30}{0.38} \\ \\
         &= 1/1.27 = 0.79
\end{align}
]

.full-width[
Tanto $\theta$ como $1/\theta$ expresan el .bold[mismo grado de asociación].
]

---
### Propiedades de la Odds Ratio

2) A diferencia de las otras medidas, la odds ratio no varia en función de que  variable actúa como dependiente e independiente. En otras palabras, no es necesario identificar una variable independiente para estimar correctamente $\theta$

--

En nuestro ejemplo, tomando género como variable dependiente, donde $\hat{p}_{A}$ es la probabilidad de ser hombre entre personar que han tenido un affair y $\hat{p}_{NA}$ es la misma probabilidad para personas que nunca han tenido un affair, la odd-ratio de ser hombre es:

.pull-left[
```{r, echo=FALSE}
prop.table(ctable,2)
```
]

.pull-right[
\begin{align}
  \hat{\theta} &= \frac{\hat{p}_{A}/(1 - \hat{p}_{A})}{\hat{p}_{NA}/(1 - \hat{p}_{NA})} \\ \\
         &= \frac{0.52/0.48}{0.46/0.54} \\ \\
         &= \frac{1.1}{0.85} \\ \\
         &= 1.27
\end{align}
]

---
### Propiedades de la Odds Ratio

3) La Odds Ratio es .bold[margins-free]: la odds ratio de una tabla de contingencia no se ven alteradas por el "escalamiento" (multiplicación por una constante) de filas o columnas.  

--

.pull-top[
.pull-left[
.bold[Movilidad educacional 1980]
```{r, echo=FALSE}
pais_1980 <- matrix(c(160,20,20,20),2,2)
colnames(pais_1980) <- c("Hij@:NU","Hij@:U")
rownames(pais_1980) <- c("Padre:NU","Padres:U")
pais_1980
```
]
.pull-right[
.bold[Movilidad educacional 2020]
```{r, echo=FALSE}
pais_2020 <- matrix(c(160,20,80,80),2,2)
colnames(pais_2020) <- c("Hij@:NU","Hij@:U")
rownames(pais_2020) <-  c("Padre:NU","Padres:U")
pais_2020
```
]
]

--

.pull-bottom[
.pull-left[
- El .bold[13%] de los hijos padres sin estudios universitarios obtenía un grado universitario

- El .bold[50%] de los hijos con padres con estudios universitarios obtenía también un grado universitario
]
.pull-right[
- El .bold[33%] de los hijos padres sin estudios universitarios obtiene un grado universitario

- El .bold[80%] de los hijos con padres con estudios universitarios obtiene también un grado universitario
]
]

.full-width[
.bold[Titular del diario:] _Aumentan oportunidades educacionales, especialmente para la así llamada "primera generación"_
]

--

.bold[Correcto?]

---
### Propiedades de la Odds Ratio


.bold[Correcto, pero engañoso:] el resultado refleja un .bold[cambio en la distribución marginal] de educación de los hijos, no un cambio en la asociación de las variables. 
--
 Concretamente, se duplicó la cantidad de gente que termina la universidad, independiente de su origen. 

.pull-top[

.pull-left[
.bold[Movilidad educacional 1980]
```{r, echo=FALSE}
pais_1980 <- matrix(c(160,20,20,20),2,2)
colnames(pais_1980) <- c("Hij@:NU","Hij@:U")
rownames(pais_1980) <- c("Padre:NU","Padres:U")
pais_1980
```
]
.pull-right[
.bold[Movilidad educacional 2020]
```{r, echo=FALSE}
pais_2020 <- matrix(c(160,20,80,80),2,2)
colnames(pais_2020) <- c("Hij@:NU","Hij@:U")
rownames(pais_2020) <-  c("Padre:NU","Padres:U")
pais_2020
```
]

]

<br>
--

.pull-bottom[

.full-width[La odds ratio es inmune a cambios en la distribución marginal de las variables, capturando sólo la asociación neta entre ellas ("margin-free association")] 

.pull-left[
$\hat{\theta}_{1980} =  \frac{160 \times 20}{20 \times 20} = 8$
]
.pull-right[
$\hat{\theta}_{2020} =  \frac{160 \times 80}{20 \times 80} = \frac{160 \times (4 \times 20)}{20 \times (4 \times 20)} = 8$
]
]
 
---
### Log Odds Ratio

Como sabemos, $\theta \in [0,\infty+)$. Esto crea un problema tanto para la .bold[interpretación] como para la .bold[inferencia estadística]. Por ejemplo:

- Supongamos que la odds ratio (hombres a mujeres) de tener un affair es $\theta = 20$.
- Por ende, la odds ratio (mujeres a hombres) de tener un affair es $\theta^{*} = 1/ \theta = 0.05$. 
- Ambos resultados indican el .bold[mismo nivel de asociación], pero uno parece mucho más grande que el otro.

--

Transformando $\theta$ a escala logarítmica permite mapear  $[0,\infty+) \to (-\infty,\infty+)$, creando una medida de asociación  simétrica. 

\begin{align}
  \theta &=  \frac{1}{\theta^{*}}  \quad \text{entonces} \\ \\
  \log(\theta) &= -1 \times \log(\theta^{*})
\end{align}

--
En nuestro ejemplo:
.pull-left[
```{r}
log(20)
```
]

.pull-right[
```{r}
log(0.05)
```
]

---
### Log Odds Ratio

.pull-left[
-  $\log(\theta) \in (\infty-,\infty+)$ 

- $\theta=0$ indica igualdad de odds y, por lo tanto, independencia

- $\log(\theta) > 0$ indica que el éxito es más probable para el grupo en el numerador

- $\log(\theta) < 0$ indica que el éxito es más probable para el grupo en el denominador

- $\lvert \log(\theta) \rvert$ indica la fuerza de la asociación entre las variables

- Valores lejos de 0, en cualquier dirección, representan fuerte evidencia contra independencia

]

.figure-right[
```{r, echo=FALSE}
units <- tibble(odds = seq(from=0, to=100, by=0.1)) %>% mutate(log_odds = log(odds))

odds_logodds <- units %>% ggplot(aes(x=odds, y=log_odds, colour="")) + geom_line() +
    scale_color_viridis_d() + guides(fill=FALSE, color=FALSE) +
    labs(x="Odds ratio", y="log Odds ratio", title = "Odds ratio to log Odds ratio")
odds_logodds
```
]

---
### Relación entre Odds ratio y Riesgo Relativo

---
### No free lunch 

![paper](paper_22table.png)
---
### Inferencia para la Odds Ratio

¿Como podemos saber si nuestros resultados no son producidos por el el mero azar?
--
Para responder esta pregunta debemos conocer la .bold[sampling distribution] de nuestro estimador, especialmente su _variabilidad_.

<br>

- El caso canónico es el promedio muestral, para el cual sabemos que: $\bar{X} \sim \mathcal{N}(\mu,\frac{\sigma}{\sqrt{n}})$
 - La desviación estándar del estimador (en este caso, $\frac{\sigma}{\sqrt{n}}$ ) es lo que denominamos .bold[error estándar (SE)].

--

 - Por qué? Si $x_1, \dots, x_n$ son _iid_, entonces:
\begin{align}
\mathbb{Var}\big(\bar{X}\big) &= \mathbb{Var}\Big(\frac{x_{i} + ... + x_{n}}{n}\Big) = \frac{1}{n^2} \Big(\mathbb{Var}(x_{i}) + ... + \mathbb{Var}(x_{n})\Big) \\
 &= \frac{1}{n^2}( \sigma^2 + ... + \sigma^2 ) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n} 
\end{align}

<br>
--

- Cual es la .bold[sampling distribucion] de $\hat{\theta}$?
  - Complicado ...


---
### Inferencia para la Odds Ratio

- Cual es la .bold[sampling distribucion] de $\hat{\theta}$? 

--
  - Cual es el error estándar de $\hat{\theta}$? Es decir,  $\mathbb{Var}\Big(\frac{n_{11}n_{22}}{n_{12}n_{21}}\Big)$ ?
  - Complicado, función no lineal de n's.
  
--
 
- Más conveniente hacer inferencia sobre $\log \hat{\theta}$

  - $\log \hat{\theta} = \log \frac{n_{11}n_{22}}{n_{12}n_{21}} = \log n_{11} + \log n_{22} - \log n_{12} - \log n_{21}$

--

- Importante resultado teórico: la sampling distribution de $\log \hat{\theta}$ es _asintóticamente_ normal:

$$\log(\hat{\theta}) \sim \mathcal{N}(\mu,\sigma)$$

 - $\mu = \log \theta$ 
 
 - $\sigma = \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}$

---
### Inferencia para la Odds Ratio

#### Intervalo de confianza para log Odds ratio

Podemos usar este resultado para construir un intervalo de confianza para el log Odds ratio, al 95% de confianza, de la siguiente manera:

\begin{align}
  95\% \text{ CI}_{\log \hat{\theta}} &= \log \hat{\theta} \pm 1.96 \times SE \\ \\
          &= \log \hat{\theta} \pm 1.96 \times \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}} } 
\end{align}

#### Intervalo de confianza para Odds ratio

Podemos obtener un intervalo de confianza estándar para la Odds ratio, al 95% de confianza, de la siguiente manera:

\begin{align}
  95\% \text{ CI}_{\hat{\theta}} &= e^{\log \hat{\theta} \pm 1.96 \times \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}} } 
\end{align}

---
### Inferencia para la Odds Ratio
#### Ilustración via Monte Carlo simulation

Escenario:
- Tenemos datos de población de las variables binarias $X$ e $Y$.
- Creamos tabla de contingencia y calculamos  $\theta$ y $\log \theta$.
- Éstos son los parámetros "verdaderos", no estimaciones. 

```{r, echo=FALSE}
x <- c(rep(1,450),rep(0,550))  
y <- rep(c(rep(1,270),rep(0,230)),2)
population_data <- data.frame(x=x,y=y)
```

```{r}
ct <- table(population_data$x,population_data$y); print(ct)

theta     = (ct[1,1]*ct[2,2])/(ct[1,2]*ct[2,1])
log_theta = log(theta)

paste0("Theta= ",round(theta,2),"; log Theta = ",round(log_theta,2))
```

---
#### Ilustración via Monte Carlo simulation


- Supón que ahora  tenemos acceso a una muestra aleatoria de los datos poblacionales
- Creamos una tabla de contingencia y calculamos  $\hat{\theta}$ y $\log \hat{\theta}$
- Éstas son nuestras estimaciones ...

--

.pull-left[
```{r}
set.seed(1234)
sample_data <- sample_n(population_data, size=500, replace=TRUE)
ct_sample <- table(sample_data$x,sample_data$y); print(ct_sample)
```
]
.pull-right[
```{r}
theta_hat = (ct_sample[1,1]*ct_sample[2,2])/(ct_sample[1,2]*ct_sample[2,1])
log_theta_hat = log(theta_hat)

paste0("Theta_hat= ",round(theta_hat,2),"; log Theta_hat = ",round(log_theta_hat,2))
```
]

<br>
--

- Estos valores son similares a los "verdaderos" parámetros, pero no exactamente los mismos
- Si calculáramos $\hat{\theta}$ y $\log \hat{\theta}$ en una muestra diferente, obtendríamos un resultado diferente
- Si pudiéramos tomar una infinidad de muestras y repetir el proceso, observaríamos la _distribución_ de  $\hat{\theta}$ y $\log \hat{\theta}$.

---
#### Ilustración via Monte Carlo simulation

- Podemos reproducir el proceso teórico de tomar infinitas muestras usando simulaciones.
- Tomas 5000 muestras aleatorias de los datos poblacionales
- Estimaremos $\hat{\theta_{1}} \dots \hat{\theta}_{5000}$ y $\log \hat{\theta_{1}} \dots \log \hat{\theta}_{5000}$


```{r}
  samp_dist_theta    <- NULL; samp_dist_logtheta <- NULL
  
  for (i in 1:5000) {
    sample_data <- sample_n(population_data, size=500, replace=TRUE)
    ct_sample <- table(sample_data$x,sample_data$y)
    theta_hat     = (ct_sample[1,1]*ct_sample[2,2])/(ct_sample[1,2]*ct_sample[2,1])
    log_theta_hat = log(theta_hat)
    samp_dist_theta[i]    <-  theta_hat
    samp_dist_logtheta[i] <-  log_theta_hat
  }

samp_dist_theta %>% glimpse();  samp_dist_logtheta %>% glimpse()
```

---
#### Ilustración via Monte Carlo simulation

La distribución de nuestras estimaciones de $\hat{\theta}$ y $\log \hat{\theta}$ se ven así:

.pull-left[
```{r, echo=FALSE, fig.width=7, fig.height=6}
ci95 = round(quantile(samp_dist_theta, p=c(0.025,0.975)),2)

samp_dist_logtheta %>% as_tibble() %>% ggplot(aes(x=value)) + 
      geom_density(colour="black", fill="black", alpha=0.1) +
      scale_color_viridis_d() + 
      geom_vline(aes(xintercept = log_theta, colour=paste0("Theta=",round(theta,2)) )) +
      geom_vline(aes(xintercept = log_theta_hat, colour=paste0("Theta_hat=",round(theta_hat,2)) )) +
      geom_vline(aes(xintercept = quantile(samp_dist_theta, p=0.025),
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed") +
      geom_vline(aes(xintercept = quantile(samp_dist_theta, p=0.975), 
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed") +
      guides(fill=FALSE) + labs(colour="", x="Theta_hat") +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="bottom") +
      guides(color=guide_legend(nrow=3,byrow=TRUE))
```
]

.pull-right[
```{r, echo=FALSE, fig.width=7, fig.height=6}
ci95 = round(quantile(samp_dist_logtheta, p=c(0.025,0.975)),2)

samp_dist_logtheta %>% as_tibble() %>% ggplot(aes(x=value)) + 
      geom_density(colour="black", fill="black", alpha=0.1) +
      scale_color_viridis_d() + 
      geom_vline(aes(xintercept = log_theta, colour=paste0("log Theta=",round(log_theta,2)) )) +
      geom_vline(aes(xintercept = log_theta_hat, colour=paste0("log Theta_hat=",round(log_theta_hat,2)) )) +
      geom_vline(aes(xintercept = quantile(samp_dist_logtheta, p=0.025),
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed") +
      geom_vline(aes(xintercept = quantile(samp_dist_logtheta, p=0.975), 
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed") +
      guides(fill=FALSE) + labs(colour="", x="log Theta_hat") +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="bottom") +
      guides(color=guide_legend(nrow=3,byrow=TRUE))
```
]

---
#### Ilustración via Monte Carlo simulation

--

- SE y 95% CI $\log \hat{\theta}$ en simulaciones 

```{r}
se_logtheta = sd(samp_dist_logtheta)
ci_logtheta = quantile(samp_dist_logtheta, p=c(0.025,0.975));
print(c(SE=se_logtheta, ci_ll=ci_logtheta[1], ci_ll=ci_logtheta[2]))
```

--

- SE y 95% CI $\log \hat{\theta}$ basados en aproximación teórica y datos muestrales

```{r}
se_logtheta_hat <- sqrt(1/ct_sample[1,1] + 1/ct_sample[1,2] + 1/ct_sample[2,1] + 1/ct_sample[2,2])
ci_logtheta_hat <- c(log_theta_hat - 1.96*se_logtheta_hat, log_theta_hat + 1.96*se_logtheta_hat)
print(c(SE=se_logtheta_hat, ci_ll=ci_logtheta_hat[1], ci_ll=ci_logtheta_hat[2]))
```

---
#### Ilustración via Monte Carlo simulation


- 95% CI $\hat{\theta}$ en simulaciones 

```{r}
ci_theta = quantile(samp_dist_theta, p=c(0.025,0.975));
print(c(ci_ll=ci_theta[1], ci_ll=ci_theta[2]))
```

<br>
--

- 95% CI $\hat{\theta}$ basados en aproximación teórica y datos muestrales

```{r}
ci_theta_hat = exp(ci_logtheta_hat); ci_theta_hat
```

<br>
--

.bold[Conclusión]: asociación entre variables es estadísticamente significativa 

---
class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca





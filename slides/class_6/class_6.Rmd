---
title: "Análisis de Datos Categóricos (SOC3070)"
subtitle: "Clase #6"
author: "<br> Mauricio Bucca<br> Profesor Asistente, Sociología UC"
date: "[github.com/mebucca](https://github.com/mebucca)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default","default-fonts","gentle-r.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
---

class: inverse, center, middle

#Tablas de Contingencia
## Medidas de asociación

---
##  Asociación en tablas de contingencia 

Las variables de una tabla de contingencia están asociadas si la distribución condicional de las variables es distinta de su distribución marginal. Formalmente, 

<br>

- $f_{Y \mid X}(Y \mid X) \neq f_{Y}(Y)$

y por tanto,

- $f_{X \mid Y}(X \mid Y) \neq f_{X}(X)$


---
##  Asociación en tablas de contingencia 

```{r,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library("tidyverse")
library("Ecdat")
library("cowplot")
theme_set(theme_cowplot())

data(Fair)
affairsdata <- Fair %>% as_tibble()

# create a binary variable indicating wether persons has ever had an affair
affairsdata <- affairsdata %>% 
	mutate(everaffair = case_when(nbaffairs == 0 ~ "Never", nbaffairs > 0 ~ "At least once") )

ctable <- affairsdata %>% with(table(sex,everaffair))
```

Continuando con nuestro ejemplo,

.pull-left[
$f(\text{everaffair} \mid \text{sex})$
```{r}
prop.table(ctable,1)
```


$f(\text{everaffair})$
```{r}
prop.table(apply(ctable,2,sum))
```

]

--

.pull-right[
Al parece que los hombres tienen una mayor probabilidad que las mujeres de haber tenido un "affair".

En lo que sigue vamos a usar este ejemplo para estudiar:

- Diferentes formas de cuantificar la asociación (o la ausencia de la misma) entre variables de una tabla de contingencia

- Evaluar si las diferencias observadas son o no más sustanciales de lo se esperaría debido al mero azar.
]

---
class: inverse, center, middle

## Medidas de Asociación

---
## Diferencia de proporciones

- Supongamos que tenemos una tabla de contingencia 2-ways que cruza las variables binarias $X$ (independiente) y $Y$ (dependiente).  Éxito se codifica con valor 1 y el fracaso con el valor 0.

- Para detectar la asociación necesitamos medir diferencias en la distribución de $Y$ condicional en $X$

--
La diferencia de proporciones cuantifica estas diferencias de la siguiente manera:

$$\delta = \mathbb{P}(Y=1 \mid X=1) - \mathbb{P}(Y=1 \mid X=0)$$
--

Noten que $\delta \in [-1,1]$ donde $\delta=0$ indica proporciones iguales. 

--

Volviendo a nuestro ejemplo, $\hat{p}_{H}$, llamemos y a la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ a la proporción de mujeres que han tenido una aventura. La diferencia de proporciones se define simplemente como:

\begin{align}
  \hat{\delta} &= \hat{p}_{H} - \hat{p}_{M} \\ \\
  &= 0.273 - 0.229 \\ \\
  &= 0.044
\end{align}

---
## Diferencia de proporciones

Dos consideraciones importantes:

1) La diferencia de proporciones debe estar adecuadamente definida en términos de una variable dependiente y otra independiente. La razón es que, en general:

$$\mathbb{P}(Y=1 \mid X=1) - \mathbb{P}(Y=1 \mid X=0) \neq  \mathbb{P}(X=1 \mid Y=1) - \mathbb{P}(X=1 \mid Y=0)$$

--

En nuestro ejemplo:

```{r}
prop.table(ctable,2)
```

Si tratamos género como variable dependiente y definimos "mujeres" como la categoría de éxito, la diferencia en las proporciones es $\delta = 0.48 - 0.54 = -0.06$. 

---
## Diferencia de proporciones

2) La diferencia de proporciones es una estadística intuitiva y fácil de interpretar, pero por sí sola puede ser engañosa cuando las proporciones son ambas cercanas a cero. Consideremos los dos casos hipotéticos siguientes:

\begin{align}
  \text{Caso 1: } p_{1a}=0.410 \text{ and } p_{1b}=0.401  \\ \\
  \text{aquí: } \delta_{1} = 0.009
\end{align}

--
y

\begin{align}
  \text{Caso 2: } p_{2a}=0.010 \text{ and } p_{2b}=0.001  \\ \\
  \text{aquí: } \delta_{2} = 0.009
\end{align}

--
¿Problemas? En el caso 1 ambas porciones son, según todos los indicios, casi idénticas. En el caso 2, sin embargo, ambas proporciones son similares en términos absolutos, por muy diferentes en términos relativos: $0.010$ es 10 veces mayor $0.001$.


---
### Riesgo Relativo (RR)

En casos como el descrito anteriormente el ratio entre las proporciones es una estadística más relevante. El riesgo relativo se define como:

$$RR = \frac{\mathbb{P}(Y=1 \mid X=1)}{\mathbb{P}(Y=1 \mid X=0)}$$

Notar que $RR \in [0,\infty+]$, donde $RR=1$ indica igualdad de proporciones. 

<br>
--

En nuestro ejemplo, el riesgo relativo estimado es:

\begin{align}
  \hat{RR} &= \frac{\hat{p}_{H}}{\hat{p}_{M}} \\ \\
  &= \frac{0.273}{0.229} = 1.19214
\end{align}

--

La proporción de infidelidad entre los hombres es aproximadamente un 20% mayor que entre las mujeres. 

---
## Riesgo Relativo (RR)

Tres consideraciones importantes:

1) Al igual que la diferencia de proporciones, el riesgo relativo debe definirse adecuadamente en términos de una variable dependiente y otra independiente. En general:

$$\frac{\mathbb{P}(Y=1 | X=1)}{\mathbb{P}(Y=1 | X=0)} \neq  \frac{\mathbb{P}(X=1 | Y=1) }{\mathbb{P}(X=1 | Y=0)}$$

--

En nuestro ejemplo:

```{r}
prop.table(ctable,2)
```

Si tratamos género como variable dependiente y definimos "mujeres" como la categoría de éxito, la diferencia en las proporciones es $RR = 0.48/0.54 = 0.89$. 

---
### Riesgo Relativo (RR)

2) El riesgo relativo depende que categoría definimos cómo "éxito". En general, 


$$\frac{\mathbb{P}(Y=1 | X=1)}{\mathbb{P}(Y=1 | X=0)} \neq \frac{\mathbb{P}(Y=0 | X=0)}{\mathbb{P}(Y=0 | X=1)}$$
--

En nuestro ejemplo:

.pull-left[
ratio proporción infidelidad H/M
\begin{align}
  \frac{\hat{p}_{H}}{\hat{p}_{M}} &= \frac{0.273}{0.229} \\ \\
   &= 1.19214
\end{align}
]

.pull-left[
ratio proporción fidelidad M/H
\begin{align}
  \frac{1-\hat{p}_{M}}{1-\hat{p}_{H}} &= \frac{0.771}{0.727} \\ \\
   &= 1.060523
\end{align}
]


---
## Riesgo Relativo (RR)

3)  Si una de las proporciones involucradas en el cálculo es demasiado pequeña, el $RR$ puede tomar valores arbitrariamente grandes o arbitrariamente pequeños. 

<br>

  - Ejemplo: $0.7/0.005 = 140$

--

  - En estos casos la definición de la variable dependiente/independiente, y de la la categoría de éxito afectan         
radicalmente el resultado:

    - invirtiendo la categoría de exito: $0.3/0.9 = 0.3$
    
    - invirtiendo la variable dependiente: $0.009/0.768=0.012$, u otros, dependiente de categoría de éxito
  
--

  - Problema muy común cuando se trabaja con eventos con muy baja prevalencia (ej, suicidio, covid, etc.)

---
## Odds Ratio

<br>

- Odds ratio ( $\theta$ ) es una medida fundamental de asociación. 

- Parámetro de interés en el modelo más importante de datos categóricos: regresión logística.

--

- $\theta$ está formulada para tablas de 2-por-2, pero también puede calcularse para tablas de mayor dimensión:

  - Toda tabla $n$-ways, $I \cdot J$, puede ser re-escrita como  $(I-1) \cdot (J-1) \cdot (n-1)$ tablas de 2-por-2.

---
### Odds

La Odds Ratio es el ratio de dos "odds", donde las "odds" una variable binaria $Y$ se definen como: 

<br>

\begin{align}
  \text{odds} &= \frac{\mathbb{P}(Y=1)}{1-\mathbb{P}(Y=1)} \\ \\
              &=  \frac{p}{1-p}
\end{align}

<br>
--

Por ejemplo, si $Y$ tiene una probabilidad de éxito $p=0.75$, las odds de éxito son $\text{odds}=\frac{0.75}{0.25} = 3$

- Esto significa que las "chances" éxito son 3:1


---
### Odds

las Odds son funciones de probabilidades y, por lo tanto, las probabilidades también pueden expresarse en función de las odds. Formalmente:

$$p = \frac{\text{odds}}{1 + \text{odds}}$$

--

Siguiendo con ejemplo anterior, si sabemos que las odds de éxito son igual a 3, entonces la probabilidad ( $p$ ) de éxito es:

.pull-left[
\begin{align}
p &= \frac{3}{1 + 3} \\ \\
  &= 0.75
\end{align}
]

--

.pull-right[

.content-box-blue[
.tiny[.bold[Derivación]:
\begin{align}
  \text{odds} &= \frac{p}{1-p}  \text{  } \\ \\
  \text{odds} &= \frac{1}{\frac{1}{p} - 1}  \\ \\
  \frac{1}{p} &= \frac{1}{\text{odds}} + 1  \\  \\
  \frac{1}{p} &= \frac{1 + \text{odds}}{\text{odds}}  \\ \\
           p  &= \frac{\text{odds}}{1 + \text{odds}}
\end{align}
]

]
]

---
## Odds Ratio

Las .bold[odds] resumen la distribución de una sola variable binaria. Para medir la asociación entre dos de estas variables en una tabla podemos calcular la .bold[odds *ratio*]. 

--

Si $X$ e $Y$ son las variables binarias -- independiente y dependiente -- la distribución condicional $f(Y \mid X)$ se puede resumir con dos .bold[odds]:

\begin{align}
  \text{odds}_{0} &=  \frac{\mathbb{P}(Y=1 | X=0) }{1 - \mathbb{P}(Y=1 | X=0) } \quad \text{y} \\ \\
  \text{odds}_{1} &=  \frac{\mathbb{P}(Y=1 | X=1) }{1 - \mathbb{P}(Y=1 | X=1) } 
\end{align}

--

La .bold[odds *ratio*], por tanto, es:

\begin{align}
  \theta &= \frac{\text{odds}_{1}}{\text{odds}_{0}} \\ \\\
\end{align}


---
## Odds Ratio

Volviendo a nuestro ejemplo,

```{r,echo=FALSE}
prop.table(ctable,1)
```

Si $\hat{p}_{H}$ es la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ es la proporción de mujeres que han tenido una aventura. 


\begin{align}
  \hat{\theta} = \frac{\text{odds}_{H}}{\text{odds}_{M}} &= \\
         &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
         &= \frac{0.273/0.727}{0.229/0.771} \\ \\
         &= \frac{0.38}{0.30} = 1.27
\end{align}

---
## Odds Ratio

Dado que estas proporciones se estiman a partir de los recuentos de la tabla, $\theta$ también puede expresarse de la siguiente manera, denominada .bold[cross-product ratio].

En nuestro ejemplo,

.pull-left[
```{r, echo=FALSE}
ctable
```
]

.pull-right[
\begin{align}
  \hat{\theta} &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
   &= \frac{\frac{n_{21}}{n_{2+}} / \frac{n_{22}}{n_{2+}}}{\frac{n_{11}}{n_{1+}}/ \frac{n_{12}}{n_{1+} }} = \frac{n_{21}/n_{22}}{n_{11}/n_{12}}  \\ \\
   &= \frac{n_{21} \cdot n_{12}}{n_{22} \cdot n_{11}} \\ \\
\end{align}
]

--

```{r}
Theta = (ctable[2,1]*ctable[1,2])/(ctable[2,2]*ctable[1,1]); Theta
```

---
## Odds Ratio

```{r}
Theta = (ctable[2,1]*ctable[1,2])/(ctable[2,2]*ctable[1,1]); Theta
```

.bold[Interpretación]: las odds de que un hombre tenga affair son 1,27 veces mayores que las de una mujer, es decir, 27% más altas. 

Notice that:

- $\theta \in [0,\infty+]$

--

- $\theta=1$ indica igualdad de odds y, por lo tanto, independencia

--

- $\theta > 1$ indica que el éxito es más probable para el grupo en el numerador (hombres en este caso)

--

- $\theta < 1$ indica que el éxito es más probable para el grupo en el denominador (mujeres en este caso)

--

- Valores lejos de 1, en cualquier dirección, representan una fuerte evidencia contra independencia

---
### Propiedades de la Odds Ratio

1) Invirtiendo el orden de las filas o columnas obtenemos el inverso la odds ratio original.

```{r,echo=FALSE}
prop.table(ctable,1)
```

Si $\hat{p}_{H}$ es la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ es la proporción de mujeres que han tenido una aventura. 

.pull-left[
\begin{align}
  \hat{\theta}_{HM} &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
         &= \frac{0.38}{0.30} \\ \\
         &= 1.27
\end{align}
]

.pull-right[
\begin{align}
  \hat{\theta}_{HM} &= \frac{\hat{p}_{M}/(1 - \hat{p}_{M})}{\hat{p}_{H}/(1 - \hat{p}_{H})} \\ \\
         &= \frac{0.30}{0.38} \\ \\
         &= 1/1.27 = 0.79
\end{align}
]

.full-width[
Tanto $\theta$ como $1/\theta$ expresan el .bold[mismo grado de asociación].
]

---
### Propiedades de la Odds Ratio

2) A diferencia de las otras medidas, la odds ratio no varia en función de que  variable actúa como dependiente e independiente. En otras palabras, no es necesario identificar una variable independiente para estimar correctamente $\theta$

--

En nuestro ejemplo, tomando género como variable dependiente, donde $\hat{p}_{A}$ es la probabilidad de ser hombre entre personar que han tenido un affair y $\hat{p}_{NA}$ es la misma probabilidad para personas que nunca han tenido un affair, la odd-ratio de ser hombre es:

.pull-left[
```{r, echo=FALSE}
prop.table(ctable,2)
```
]

.pull-right[
\begin{align}
  \hat{\theta} &= \frac{\hat{p}_{A}/(1 - \hat{p}_{A})}{\hat{p}_{NA}/(1 - \hat{p}_{NA})} \\ \\
         &= \frac{0.52/0.48}{0.46/0.54} \\ \\
         &= \frac{1.1}{0.85} \\ \\
         &= 1.27
\end{align}
]

---
### Propiedades de la Odds Ratio

3) La Odds Ratio es .bold[margins-free]: la odds ratio de una tabla de contingencia no se ven alteradas por el "escalamiento" (multiplicación por una constante) de filas o columnas.  

--

.pull-top[
.pull-left[
.bold[Movilidad educacional 1980]
```{r, echo=FALSE}
pais_1980 <- matrix(c(160,20,20,20),2,2)
colnames(pais_1980) <- c("Hij@:NU","Hij@:U")
rownames(pais_1980) <- c("Padre:NU","Padres:U")
pais_1980
```
]
.pull-right[
.bold[Movilidad educacional 2020]
```{r, echo=FALSE}
pais_2020 <- matrix(c(160,20,80,80),2,2)
colnames(pais_2020) <- c("Hij@:NU","Hij@:U")
rownames(pais_2020) <-  c("Padre:NU","Padres:U")
pais_2020
```
]
]

--

.pull-bottom[
.pull-left[
- El .bold[13%] de los hijos padres sin estudios universitarios obtenía un grado universitario

- El .bold[50%] de los hijos con padres con estudios universitarios obtenía también un grado universitario
]
.pull-right[
- El .bold[33%] de los hijos padres sin estudios universitarios obtiene un grado universitario

- El .bold[80%] de los hijos con padres con estudios universitarios obtiene también un grado universitario
]
]

--

.full-width[
.bold[Titular del diario:] _Aumentan oportunidades educacionales, especialmente para la así llamada "primera generación"_
]

--

.bold[Correcto?]

---
### Propiedades de la Odds Ratio


.bold[Correcto, pero parcialmente:] el resultado refleja un .bold[cambio en la distribución marginal] de educación de los hijos, no un cambio en la asociación de las variables. 
--
 Concretamente, se duplicó la cantidad de gente que termina la universidad, independiente de su origen. 

.pull-top[

.pull-left[
.bold[Movilidad educacional 1980]
```{r, echo=FALSE}
pais_1980 <- matrix(c(160,20,20,20),2,2)
colnames(pais_1980) <- c("Hij@:NU","Hij@:U")
rownames(pais_1980) <- c("Padre:NU","Padres:U")
pais_1980
```
]
.pull-right[
.bold[Movilidad educacional 2020]
```{r, echo=FALSE}
pais_2020 <- matrix(c(160,20,80,80),2,2)
colnames(pais_2020) <- c("Hij@:NU","Hij@:U")
rownames(pais_2020) <-  c("Padre:NU","Padres:U")
pais_2020
```
]

]

<br>
--

.pull-bottom[

.full-width[La odds ratio es "inmune a cambios" en la distribución marginal de las variables, capturando sólo la asociación neta entre ellas ("margin-free association")] 

.pull-left[
$\hat{\theta}_{1980} =  \frac{160 \cdot 20}{20 \cdot 20} = 8$
]
.pull-right[
$\hat{\theta}_{2020} =  \frac{160 \cdot 80}{20 \cdot 80} = \frac{160 \cdot (4 \cdot 20)}{20 \cdot (4 \cdot 20)} = 8$
]
]
 
---
### Log Odds Ratio

Como sabemos, $\theta \in [0,\infty+)$. Esto crea un problema tanto para la .bold[interpretación] como para la .bold[inferencia estadística]. Por ejemplo:

- Supongamos que la odds ratio (hombres a mujeres) de tener un affair es $\theta = 20$.
- Por ende, la odds ratio (mujeres a hombres) de tener un affair es $\theta^{*} = 1/ \theta = 0.05$. 
- Ambos resultados indican el .bold[mismo nivel de asociación], pero uno parece mucho más grande que el otro.

--

Transformando $\theta$ a escala logarítmica permite mapear  $[0,\infty+) \to (-\infty,\infty+)$, creando una medida de asociación  simétrica. 

\begin{align}
  \theta &=  \frac{1}{\theta^{*}}  \quad \text{entonces} \\ \\
  \log(\theta) &= -1 \cdot \log(\theta^{*})
\end{align}

--
En nuestro ejemplo:
.pull-left[
```{r}
log(20)
```
]

.pull-right[
```{r}
log(0.05)
```
]

---
### Log Odds Ratio

.pull-left[
-  $\log(\theta) \in (\infty-,\infty+)$ 

- $\theta=0$ indica igualdad de odds y, por lo tanto, independencia

- $\log(\theta) > 0$ indica que el éxito es más probable para el grupo en el numerador

- $\log(\theta) < 0$ indica que el éxito es más probable para el grupo en el denominador

- $\lvert \log(\theta) \rvert$ indica la fuerza de la asociación entre las variables

- Valores lejos de 0, en cualquier dirección, representan fuerte evidencia contra independencia

]

.pull-right[
```{r, echo=FALSE, fig.width= 6}
units <- tibble(odds = seq(from=0, to=100, by=0.1)) %>% mutate(log_odds = log(odds))

odds_logodds <- units %>% ggplot(aes(x=odds, y=log_odds, colour="")) + geom_line(size=1.5) +
    scale_color_viridis_d() + guides(fill=FALSE, color=FALSE) +
    labs(x="Odds ratio", y="log Odds ratio", title = "Odds ratio to log Odds ratio") +
    theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
    legend.text = element_text(size = 18), legend.position="none") 

  print(odds_logodds)
```
]

---
### No free lunch 

![paper](paper_22table.png)


---
### Relación entre Odds ratio y Riesgo Relativo

.pull-left[
![meme_rr](or_rr.jpg)
]

--

.pull-right[
Se puede mostrar que al calcular las odds ratio entre dos grupos 1 y 2, 

$$\theta = RR \cdot \frac{1 - p_{2}}{1 - p_{1}}$$
<br>
Por tanto, si las proporciones $p_{1}$ y $p_{2}$ son ambas cercanas a cero, entonces $\theta \approx RR$
]

---
class: inverse, center, middle

## Inferencia para Medidas de Asociación


---
## Inferencia para medidas de Asociación

O, sobre como podemos saber si nuestros resultados no son, o no, producidos por el mero azar.

--

- Para responder esta pregunta debemos conocer la .bold[sampling distribution] de nuestro estimador, especialmente su _variabilidad_.

- Los parámetros que "generan" los datos no varían pero nuestra estimaciones si: de muestra en muestra.

--

<br>

.bold[Caso canónico] es el _promedio muestral_, para el cual sabemos que: $\bar{X} \sim \mathcal{N}(\mu,\frac{\sigma}{\sqrt{n}})$
 - La desviación estándar del estimador (en este caso, $\frac{\sigma}{\sqrt{n}}$ ) es lo que denominamos .bold[error estándar (SE)].

--

 - Por qué? Si $x_1, \dots, x_n$ son _iid_, entonces:
\begin{align}
\mathbb{Var}\big(\bar{X}\big) &= \mathbb{Var}\Big(\frac{x_{i} + ... + x_{n}}{n}\Big) = \frac{1}{n^2} \Big(\mathbb{Var}(x_{i}) + ... + \mathbb{Var}(x_{n})\Big) \\
 &= \frac{1}{n^2}( \sigma^2 + ... + \sigma^2 ) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n} 
\end{align}



---
### Inferencia para Diferencia de proporciones

Como recordarán de clases anteriores, asintóticamente, la "sampling distribution" de una proporción es:

$$\hat{p} \sim \mathcal{N}(\mu,\sigma) \quad \quad \text{ donde }\mu = p \quad \text{ y }\quad \sigma = \sqrt{p(1-p)/n}$$

--

Por tanto, la "sampling distribution" de la diferencia entre dos proporciones _independientes_,  $\hat{\delta} = \hat{p}_{1} - \hat{p}_{2}$, es:


$$\mathcal{N}\Big(\mu_{1} = p_{1}, \sigma_{1} = \sqrt{p_{1}(1-p_{1})/n_{1}}\Big) - \mathcal{N}\Big(\mu_{2} = p_{2},  \sigma_{2} = \sqrt{p_{2}(1-p_{2})/n_{2}}\Big)$$
--

dado que para variables independiente X e Y: 
  - $\mathbb{E}(X - Y)  = E(X) -  E(Y)$ y $\mathbb{Var}(X - Y)  = \mathbb{Var}(X) + \mathbb{Var}(Y)$

entonces:

--

.content-box-blue[
$$\hat{p}_{1} - \hat{p}_{2} \sim \mathcal{N}\Big(\mu_{\delta} = p_{1} - p_{2}, \sigma_{\delta} = \sqrt{p_{1}(1-p_{1})/n_{1} + p_{2}(1-p_{2})/n_{2}}\Big)$$
]

---
### Inferencia para Diferencia de proporciones

.bold[Intervalo de confianza]
Podemos usar este resultado para construir un intervalo de confianza para $\hat{\delta} = \hat{p_{1}} - \hat{p_{2}}$, al (1 - $\alpha$)% de confianza. Para un nivel de significación estadística de $\alpha=0.05$,

\begin{align}
  95\% \text{ CI}_{\hat{\delta}} &= \hat{\delta} \pm 1.96 \times SE \\ \\
          &= (\hat{p_{1}} - \hat{p_{2}}) \pm 1.96  \sqrt{p_{1}(1-p_{1})/n_{1} + p_{2}(1-p_{2})/n_{2}}
\end{align}

<br>
.bold[Nota importante]: cuando no conocemos los _verdaderos_ parámetros reemplazamos por sus valores estimados  (en este caso, $\hat{p_{1}}$ y $\hat{p_{2}}$ en vez de $p_{1}$ y $p_{2}$).


---
### Inferencia para Diferencia de proporciones

$$95\% \text{ CI}_{\hat{\delta}} = (\hat{p_{1}} - \hat{p_{2}}) \pm 1.96  \sqrt{p_{1}(1-p_{1})/n_{1} + p_{2}(1-p_{2})/n_{2}}$$

En nuestro ejemplo,

.pull-left[
```{r}
print(ctable)
```
]

.pull-right[
```{r}
n1 = sum(ctable[2,])
n2 = sum(ctable[1,])
p1_hat = ctable[2,1]/n1
p2_hat = ctable[1,1]/n2

```
]


```{r diff-prop, eval=FALSE}
delta_hat = p1_hat - p2_hat
se = sqrt((p1_hat*(1 - p1_hat))/n1 +  (p2_hat*(1 - p2_hat))/n2)
ci95_delta= c(ll=(delta_hat - 1.96*se), ul=(delta_hat + 1.96*se)); print(ci95_delta)
```

--

.pull-left[
Nuestro 95% CI:
```{r diff-prop-out, ref.label="diff-prop", echo=FALSE}
```
]

.pull-right[
Versión automática con `prop.test()` en `R`:
```{r, echo=FALSE}
test_props <- prop.test(rev(ctable[,1]), rev(apply(ctable,1,sum)), p = NULL, alternative = "two.sided", correct=FALSE)
print(test_props$conf.int[c(1,2)])
```
]

```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```

---
### Inferencia para Diferencia de proporciones (Nota)

![overlapping](overlapping.png)
* No del todo cierto, pero tiene un punto importante

---
### Inferencia para Diferencia de proporciones (Nota)

.tiny[*asumamos mismo tamaño muestral $n$ por simpleza]. Queremos saber si $\mu_{1}$ es significativamente .bold[mayor] que $\mu_{2}$.


.pull-left[
.bold[Intervalos de confianza sobrepuesto]

![diff](diff.png)


$(\mu_{1} - c \cdot \sigma_{1}/\sqrt{n}) - (\mu_{2} + c \cdot \sigma_{2}/\sqrt{n}) > 0 \quad ?$ 

es decir, 

$(\mu_{1}  - \mu_{2}) -  c \cdot (\sigma_{1}/\sqrt{n} + \sigma_{2}/\sqrt{n}) > 0 \quad ?$
]

.pull-right[
.bold[Intervalos de confianza de differencia]

$(\mu_{1}  - \mu_{2}) -  c \cdot \sqrt{(\sigma^{2}_{1}/n + \sigma^{2}_{2}/n)} > 0 \quad ?$

]


---
### Inferencia para Diferencia de proporciones

.bold[Test de hipótesis]

1) ¿Cuál es la distribución de $\hat{p}_{1} - \hat{p}_{2}$ bajo la nula (si la hipótesis nula es verdadera)?
  - $H_{0}: \tilde{\delta} = \hat{p}_{1} - \hat{p}_{2}=0$ 

--

.content-box-blue[
$$\tilde{\delta} =  \hat{p}_{1} - \hat{p}_{2} \sim \mathcal{N}\Big(\mu_{\delta} = 0, \sigma_{\delta} = \sqrt{p(1-p)(1/n_{1}+1/n_{2})}\Big)$$
]


- Típicamente $p$ se reemplaza por el promedio ponderado de $\hat{p_{1}}$ y $\hat{p_{2}}$  

<br>
--

2) Calcular .bold[p-value] (2 colas)


.content-box-blue[
$$\mathbb{P}( |\delta| > \hat{\delta} \mid H_{0} \text{ es verdadera})$$
]

---

### Inferencia para Diferencia de proporciones

.bold[Test de hipótesis]


.pull-left[
.bold[p-value] (1 cola):

$$\mathbb{P}( \tilde{\delta} \geq \hat{\delta}=0.044 \mid H_{0})$$
```{r}
p_null = p1_hat*(n1/(n1+n2)) + p2_hat*(n2/(n1+n2))
se_null = sqrt(p_null*(1 - p_null)*(1/n1 + 1/n2))
```

```{r}
1 - pnorm(delta_hat,mean=0,sd=se_null)
```
]

--

.pull-right[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

p_null = p1_hat*(n1/(n1+n2)) + p2_hat*(n2/(n1+n2))
se_null = sqrt(p_null*(1 - p_null)*(1/n1 + 1/n2))

mydata <- data_frame(x = seq(from = -0.2, to = 0.2, by =0.01), delta_undernull = dnorm(x,mean=0,sd=se_null))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
    ## Entire curve
    geom_path(aes(y=delta_undernull,color=""), size=1.5, alpha=0.8) +
  labs(y="f(y)", x="y", title="Probability function X^2 con df=1") +
     scale_color_viridis_d() +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="none") +
      geom_vline(xintercept = delta_hat, color = "blue", size=1.5) +
      annotate(geom="text", x=delta_hat+ 0.09, y=9, label='bold("nuestra dif. p: 0.044")', color="black", parse=TRUE, size=8) 

print(plot)
```
.bold[Para ser claros:] Si $H_{0}$ es cierta, nuestro $\hat{\delta}$ distribuye $\mathcal{N}(0,\alpha=p(1-p)(1/n_{1} + 1/n_{2})$
]


---
### Inferencia para la Odds Ratio

- Cual es la .bold[sampling distribucion] de $\hat{\theta}$? 

--

  - Si para un proporción sabemos que $\hat{p} \sim \mathcal{N}(\mu,\sigma) \quad \quad \text{ donde }\mu = p \quad \text{ y }\quad \sigma = \sqrt{p(1-p)/n}$
  

<br>
La sampling distribution de $\hat{\theta}$ debe ser ...

--


.pull-left[
$$\hat{\theta} \sim \frac{\frac{\mathcal{N}(\mu_{1},\sigma_{1})}{1 - \mathcal{N}(\mu_{1},\sigma_{1})}}{\frac{\mathcal{N}(\mu_{2},\sigma_{2})}{1 - \mathcal{N}(\mu_{2},\sigma_{2})}}$$  
]

--
.pull-right[
![meme](meme.png)

Complicado ...
]



  
  
---
### Inferencia para la Odds Ratio

- Más conveniente hacer inferencia sobre $\log \hat{\theta}$

- Usando la definición de $\hat{\theta}$ como cross-product, obtenemos:


  $$\log \hat{\theta} = \log \frac{n_{11}n_{22}}{n_{12}n_{21}} = \log n_{11} + \log n_{22} - \log n_{12} - \log n_{21}$$

<br>
--

Importante resultado teórico: la sampling distribution de $\log \hat{\theta}$ es _asintóticamente_ normal:

$$\log(\hat{\theta}) \sim \mathcal{N}(\mu,\sigma)$$
con parámetros _estimados_ por:

 - $\hat{\mu} = \log \hat{\theta}$ 
 
 - $\hat{\sigma} = \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}$

---
### Inferencia para la Odds Ratio

#### Intervalo de confianza para log Odds ratio

Podemos usar este resultado para construir un intervalo de confianza para el log Odds ratio, al (1 - $\alpha$) de confianza. Para un nivel de significación estadística de $\alpha=0.05$,

\begin{align}
  95\% \text{ CI}_{\log \hat{\theta}} &= \log \hat{\theta} \pm 1.96 \cdot SE \\ \\
          &= \log \hat{\theta} \pm 1.96 \cdot \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}} } 
\end{align}

--

#### Intervalo de confianza para Odds ratio

Podemos obtener un intervalo de confianza estándar para la Odds ratio, al (1 - $\alpha$) de confianza tomando el exponencial del intervalo obtenido para $\log \hat{\theta}$.

\begin{align}
  95\% \text{ CI}_{\hat{\theta}} &= e^{\log \hat{\theta} \pm 1.96 \cdot \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}} } 
\end{align}

---
### Inferencia para la Odds Ratio
#### Ilustración via Monte Carlo simulation

Escenario:
- Conocemos las verdaderas probabilidad $Y \mid X$ (ambas binarias).
- A partir de ellas calculamos  $\theta$ y $\log \theta$.
- Estos son los parámetros "verdaderos", no estimaciones. 



```{r, echo=FALSE}
probs <- matrix(c(0.6, 0.4, 0.4000000,0.6000000),2,2)
print(round(probs,2))

theta     = (probs[1,1]/probs[2,1])/(probs[1,2]/probs[2,2])
log_theta = log(theta)

paste0("Theta= ",round(theta,2),"; log Theta = ",round(log_theta,2))
```

---
#### Ilustración via Monte Carlo simulation


- Supón que ahora tenemos acceso a una _muestra_ de 500 casos generados con estas probabilidades 
- Creamos una tabla de contingencia y calculamos  $\hat{\theta}$ y $\log \hat{\theta}$
- Éstas son nuestras estimaciones ...

--

.pull-left[
```{r}
set.seed(98711)
ct_sample <- matrix(rpois(4,probs*500),2,2)
print(ct_sample)
```
]
.pull-right[
```{r}
theta_hat = (ct_sample[1,1]*ct_sample[2,2])/(ct_sample[1,2]*ct_sample[2,1])
log_theta_hat = log(theta_hat)

paste0("Theta_hat= ",round(theta_hat,2),"; log Theta_hat = ",round(log_theta_hat,2))
```
]

<br>
--

- Estos valores son similares a los "verdaderos" parámetros, pero no exactamente los mismos
- Si calculáramos $\hat{\theta}$ y $\log \hat{\theta}$ en una muestra diferente, obtendríamos un resultado diferente
- Si pudiéramos tomar una infinidad de muestras y repetir el proceso, observaríamos la _distribución_ de  $\hat{\theta}$ y $\log \hat{\theta}$.

---
#### Ilustración via Monte Carlo simulation

- Podemos reproducir el proceso teórico de tomar infinitas muestras usando simulaciones.
- Generamos 10,000 muestras aleatorias de 500 casos, generadas con las probabilidades "verdaderas"
- Estimaremos $\hat{\theta_{1}} \dots \hat{\theta}_{10000}$ y $\log \hat{\theta_{1}} \dots \log \hat{\theta}_{10000}$


```{r}
  set.seed(98711)
  samp_dist_theta    <- NULL; samp_dist_logtheta <- NULL
  
  for (i in 1:10000) {
    ct_sim <- matrix(rpois(4,probs*500),2,2)
    sim_theta_hat     = (ct_sim[1,1]*ct_sim[2,2])/(ct_sim[1,2]*ct_sim[2,1])
    sim_log_theta_hat = log(sim_theta_hat)
    samp_dist_theta[i]    <-  sim_theta_hat
    samp_dist_logtheta[i] <-  sim_log_theta_hat
  }

samp_dist_theta %>% glimpse();  samp_dist_logtheta %>% glimpse()
```

---
#### Ilustración via Monte Carlo simulation

La distribución de nuestras estimaciones de $\hat{\theta}$ y $\log \hat{\theta}$ se ven así:

.pull-left[
```{r, echo=FALSE, fig.width=7, fig.height=6}
ci95 = round(quantile(samp_dist_theta, p=c(0.025,0.975)),2)

samp_dist_logtheta %>% as_tibble() %>% ggplot(aes(x=value)) + 
      geom_density(colour="black", fill="blue", alpha=0.1, size=1.5) +
      scale_color_viridis_d() + 
      geom_vline(aes(xintercept = theta, colour=paste0("Theta=",round(theta,2)) ), size=1.5) +
      geom_vline(aes(xintercept = theta_hat, colour=paste0("Theta_hat=",round(theta_hat,2)) ), size=1.5) +
      geom_vline(aes(xintercept = quantile(samp_dist_theta, p=0.025),
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed", size=1.5) +
      geom_vline(aes(xintercept = quantile(samp_dist_theta, p=0.975), 
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed", size=1.5) +
      guides(fill=FALSE) + labs(colour="", x="Theta_hat") +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="bottom") +
      guides(color=guide_legend(nrow=3,byrow=TRUE))
```
]

.pull-right[
```{r, echo=FALSE, fig.width=7, fig.height=6}
ci95 = round(quantile(samp_dist_logtheta, p=c(0.025,0.975)),2)

samp_dist_logtheta %>% as_tibble() %>% ggplot(aes(x=value)) + 
      geom_density(colour="black", fill="blue", alpha=0.1, size=1.5) +
      scale_color_viridis_d() + 
      geom_vline(aes(xintercept = log_theta, colour=paste0("log Theta=",round(log_theta,2)) ), size=1.5) +
      geom_vline(aes(xintercept = log_theta_hat, colour=paste0("log Theta_hat=",round(log_theta_hat,2)) ), size=1.5) +
      geom_vline(aes(xintercept = quantile(samp_dist_logtheta, p=0.025),
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed", size=1.5) +
      geom_vline(aes(xintercept = quantile(samp_dist_logtheta, p=0.975), 
                     colour=paste0("95% CI: (",ci95[1],",",ci95[2],")")), linetype="dashed", size=1.5) +
      guides(fill=FALSE) + labs(colour="", x="log Theta_hat") +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="bottom") +
      guides(color=guide_legend(nrow=3,byrow=TRUE))
```
]

---
#### Ilustración via Monte Carlo simulation

--

- SE y 95% CI $\log \hat{\theta}$ en simulaciones 

```{r}
se_logtheta = sd(samp_dist_logtheta)
ci_logtheta = quantile(samp_dist_logtheta, p=c(0.025,0.975));
print(c(SE=se_logtheta, ci_ll=ci_logtheta[1], ci_ll=ci_logtheta[2]))
```

--

- SE y 95% CI $\log \hat{\theta}$ basados en aproximación teórica y datos muestrales

```{r}
se_logtheta_hat <- sqrt(1/ct_sample[1,1] + 1/ct_sample[1,2] + 1/ct_sample[2,1] + 1/ct_sample[2,2])
ci_logtheta_hat <- c(log_theta_hat - 1.96*se_logtheta_hat, log_theta_hat + 1.96*se_logtheta_hat)
print(c(SE=se_logtheta_hat, ci_ll=ci_logtheta_hat[1], ci_ll=ci_logtheta_hat[2]))
```

---
#### Ilustración via Monte Carlo simulation


- 95% CI $\hat{\theta}$ en simulaciones 

```{r}
ci_theta = quantile(samp_dist_theta, p=c(0.025,0.975));
print(c(ci_ll=ci_theta[1], ci_ll=ci_theta[2]))
```

--

- 95% CI $\hat{\theta}$ basados en aproximación teórica y datos muestrales

```{r}
ci_theta_hat = exp(ci_logtheta_hat); ci_theta_hat
```

- Versión automática en `R`

```{r, message=FALSE}
library("DescTools")
OddsRatio(ct_sample, conf.level = 0.95, method = "mle")
```

--

.bold[Conclusión]: asociación entre variables es estadísticamente significativa 

---
#### Intervalo de confianza para Odds ratio

$$95\% \text{ CI}_{\hat{\theta}} = e^{\log \hat{\theta} \pm 1.96 \cdot \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}}$$

Aplicado a nuestro ejemplo

.pull-left[
```{r}
print(ctable)
```
]

.pull-right[
```{r}
n11 = ctable[1,1]
n12 = ctable[1,2]
n21 = ctable[2,1]
n22 = ctable[2,2]
```
]


```{r theta-prop, eval=FALSE}
log_theta_hat = log((n21*n12)/(n11*n22))
se = sqrt(1/n11+ 1/n12 + 1/n21 + 1/n22)
ci95_theta_hat= c(ll=exp(log_theta_hat - 1.96*se), ul=exp(log_theta_hat + 1.96*se)); print(ci95_theta_hat)
```

--

.pull-left[
Nuestro 95% CI:
```{r theta-prop-out, ref.label="theta-prop", echo=FALSE}
```
]

.pull-right[
Versión automática con `prop.test()` en `R`:
```{r, echo=FALSE}
OddsRatio(ctable[c(2,1),], conf.level = 0.95, method = "mle")
```
]

```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```


---
### Inferencia para Riesgo Relativo

- El Riesgo Relativo es el ratio entre dos proporciones _independientes_,  $\hat{RR} = \hat{p}_{1}/\hat{p}_{2}$

- Cada $\hat{p} \sim \mathcal{N}(\mu,\sigma) \quad \text{ con } \quad \mu = p \quad \text{ y }\quad \sigma = \sqrt{p(1-p)/n}$

--

- Cual es la distribución del ratio de dos normales, $\mathcal{N}(\mu_{1},\sigma_{1})/ \mathcal{N}(\mu_{2},\sigma_{2})$ ?

  -  No-negativo y muy asimétrica (cola derecha larga)

--

- Más simple (y más fácil interpretación) realizar inferencia sobre el $\log\hat{RR} = \log \hat{p}_{1} - \log \hat{p}_{2}$


<br>
--

- Asintóticamente $\log \hat{RR} \sim \mathcal{N}(\mu,\sigma)$, con parametros _estimados_ por: 

  -  $\quad \hat{\mu}: \hat{p}_{1}/\hat{p}_{2} \quad \text{ y }\quad \hat{\sigma} = \sqrt{1/n_{11} - 1/n_{1+} + 1/n_{21} - 1/n_{2+} }$


--

- Podemos usar este resultado para construir un intervalo de confianza para el $\log RR$ al (1 - $\alpha$) de confianza.

---
### Inferencia para Riesgo Relativo

[Intergenerational Social Mobility Among the Children of Immigrants in Western Europe](https://mebucca.github.io/research/sm2geu)

.pull-left[
![paper1](SM2gEU1.png)
]

.pull-right[
![paper2](SM2gEU2.png)
]

---
class: inverse, center, middle

## Tests de Independencia 

---
### Test $\chi^{2}$ de indepencia estadística 

Primer paso, testear que exista _algo_ de asociación: ¿son estas tablas _suficientemente distintas_? 

```{r, echo=FALSE}
# joint
joint_gender_affair <- ctable/sum(ctable); 
# marginal gender 
margin_gender <- apply(joint_gender_affair,1,sum)
# marginal affair 
margin_affair <- apply(joint_gender_affair,2,sum)
```

.pull-left[
.bold[Frecuencias observadas]
```{r, echo=FALSE}
# joint_dis <- ctable/sum(ctable); joint_dis  
print(ctable)
```
]

--

.pull-right[
.bold[Frecuencias esperadas bajo independencia]
```{r, echo=FALSE}
# expected joint probs under independence 
joint_gender_affair_indep <- margin_gender %*% t(margin_affair)
rownames(joint_gender_affair_indep) <- c("female","male")
ctable_independence <- sum(ctable)*joint_gender_affair_indep 
print(ctable_independence)
```
]

Donde cada frecuencia esperada bajo independencia está dada por: $\tilde{n}_{ij} = n \cdot \hat{p}_{i+} \cdot  \hat{p}_{+j}$

--

- El test (Pearson) $\chi^{2}$ mide el grado asociación en la tabla de la siguiente manera:

.content-box-blue[
$$\text{test } \chi^{2}=\sum_{\text{all k: } i,j} \frac{(n_{ij} - \tilde{n}_{ij})^{2}}{\tilde{n}_{ij}}$$
]

Un valor alto en el test de $\chi^{2}$ sugiere que las variables no son independientes.
--
Pero, ¿cuánto es "alto"?

---
### Test $\chi^{2}$ de indepencia estadística 

.bold[Nota:]
- Si $Z_{1}, \dots , Z_{k}$ son variables independientes y cada $Z \sim \mathcal{N}(0,1)$, 
- Entonces la variable $Y = \sum_{k} Z^{2} \sim \chi^{2}_{k}$. $Y$ distribuye $\chi^{2}$ con $k$ grados de libertad.

--

.bold[Heuristica:]

- Si $X ~ \text{Binomial(n,p)}$ entonces, asintóticamente $\frac{X - np}{\sqrt{np(1-p)}}  \sim \mathcal{N}(0,1)$
- Por tanto, $\sum_{k} \frac{ (X - np)^{2} }{np(1-p)}  \sim \chi^{2}_{k}$  

--

.content-box-blue[
Pearson demostró que bajo $H_{0}, \quad$ $\text{test}\chi^{2} \sim \chi_{df}^{2}$ :
$$\sum_{\text{all } i,j} \frac{(n_{ij} - \tilde{n}_{ij})^{2}}{\tilde{n}_{ij}} = \sum_{\text{all } i,j} \frac{(n_{ij} - n\tilde{p}_{ij})^{2}}{n\tilde{p}_{ij}} \sim \chi^{2}_{df} \quad \quad \text{donde } \quad  df= (I-1)(J-1)$$

]
---
### Test $\chi^{2}$ de indepencia estadística 

.pull-left[
.bold[Frecuencias observadas]
```{r, echo=FALSE}
ctable %>% print()
```
]

.pull-right[
.bold[Frecuencias esperadas bajo independencia]
```{r, echo=FALSE}
ctable_independence %>% print()
```
]

<br>
--

.bold[(O-E)^2/E]

```{r}
(((ctable - ctable_independence)^(2))/ctable_independence) %>% print()
```


--

.bold[Test Chi-2 : ∑ (O-E)^2/E]

```{r, echo=FALSE}
our_chi2 <- (((ctable - ctable_independence)^(2))/ctable_independence) %>% sum(); print(our_chi2)
```

---
### Test $\chi^{2}$ de indepencia estadística 


.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

df=1

mydata <- data_frame(x = seq(from = 0, to = 10, by =0.01), chi2 = dchisq(x,df))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
    ## Entire curve
    geom_path(aes(y=chi2,color=""), size=1.5, alpha=0.8) +
  labs(y="f(y)", x="y", title="Probability function X^2 con df=1") +
     scale_color_viridis_d() +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="none") +
      geom_vline(xintercept = our_chi2, color = "blue", size=1.5) +
      annotate(geom="text", x=our_chi2+2.8, y=3.7, label='bold("nuestro test chi-2: 1.56")', color="black", parse=TRUE, size=8) 

print(plot)
```
.bold[Para ser claros:] Si la hipótesis de independencia ( $H_{0}$ ) es cierta, nuestro $\text{test } \chi^{2}$ distribuye $\chi^{2}$ con  $df= (I-1)(J-1)=1$
]

--

.pull-right[
.bold[p-value]

$$\mathbb{P}(\chi_{df=1}^{2} \geq \text{test } \chi^{2} \mid H_{0})$$
```{r}
1- pchisq(our_chi2,df=1)
```

]


---
### Test $\chi^{2}$ de indepencia estadística 

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

df=1

mydata <- data_frame(x = seq(from = 0, to = 10, by =0.01), chi2 = dchisq(x,df))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
    ## Entire curve
    geom_path(aes(y=chi2,color=""), size=1.5, alpha=0.8) +
  labs(y="f(y)", x="y", title="Probability function X^2 con df=1") +
     scale_color_viridis_d() +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="none") +
      geom_vline(xintercept = our_chi2, color = "blue", size=1.5) +
      annotate(geom="text", x=our_chi2+2.8, y=3.7, label='bold("nuestro test chi-2: 1.56")', color="black", parse=TRUE, size=8) 

print(plot)
```
.bold[Para ser claros:] Si la hipótesis de independencia ( $H_{0}$ ) es cierta, nuestro $\text{test } \chi^{2}$ distribuye $\chi^{2}$ con  $df= (I-1)(J-1)=1$
]


.pull-right[
.bold[p-value]

$$\mathbb{P}(\chi_{df=1}^{2} \geq \text{test } \chi^{2} \mid H_{0})$$
```{r}
1- pchisq(our_chi2,df=1)
```


```{r}
# Versión automática
chisq.test(ctable,correct = FALSE)
```

]

---
### Test Likelihood-Ratio $G^{2}$

- .bold[Recordar]:  El MLE $\hat{\beta}$ de; parámetro $\beta$ es el valor que maximiza la probabilidad de ocurrencia los datos. Decimos que tal es el valor más "plausible" del parámetro 
  - Formalmente: $\hat{\boldsymbol{\beta}}_{MLE} = \underset{\beta}{\arg\max\ } \mathcal{L}(\boldsymbol{\beta} \mid \boldsymbol{X})$
  
- El estimador es una función de los datos: $\hat{\beta}_{MLE} = f(x_{1}, x_{2}, \dots, x_{n})$

<br>
--

.bold[Test Likelihood-Ratio] $G^{2}$


$$G^{2} = -2 \log \Bigg(  \frac{\hat{\beta}_{MLE} \text{ bajo } H_{0}}{\hat{\beta}_{MLE} \text{ sin restricción}}\Bigg)$$

--

- Si $H_{0}$ es falsa el ratio generalmente es una número negativo muy cercano a cero. Por tanto, $G^2$ será un número grande y positivo.

- $G^{2} \sim \chi^{2}_{df=k} \quad \quad \text{donde k = # parametros sin restricciones - # parameteros bajo } H_{0}$


---
### Test Likelihood-Ratio $G^{2}$


.bold[Test Likelihood-Ratio] $G^{2}$, en el caso de evaluar independencia en una tabla de contingencia 2-ways:


$$G^{2} = -2 \log \Bigg(  \frac{\hat{\beta}_{MLE} \text{ bajo } H_{0}}{\hat{\beta}_{MLE} \text{ sin restricción}}\Bigg) = -2 \log \frac{f(\tilde{n}_{ij},  \dots, \tilde{n}_{IJ})}{f(n_{ij},  \dots, n_{IJ})}$$


<br>
--
En tabla de contingencia 2-ways donde las frecuencias vienen de una distribución Multinomial, :


.content-box-blue[
Pearson demostró que bajo $H_{0}, \quad$ $G^{2} \sim \chi_{df}^{2}$:

$$2 \sum_{\text{all } i,j} n_{ij} \log \Big( \frac{n_{ij}}{\tilde{n}_{ij}} \Big) \quad \quad \text{donde } \quad  df= (I
-1)(J-1)$$
]

--

- $G^{2}=0$ cuando $n_{ij}=\tilde{n}_{ij} \quad \text{ para todo } i,j$

- $G^{2}$ "grande" sugiere que variables no son independientes

---
### Test Likelihood-Ratio $G^{2}$

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

our_g2 = 2*sum(ctable*log((ctable/ctable_independence))); print(our_g2)
df=1

mydata <- data_frame(x = seq(from = 0, to = 10, by =0.01), chi2 = dchisq(x,df))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
    ## Entire curve
    geom_path(aes(y=chi2,color=""), size=1.5, alpha=0.8) +
  labs(y="f(y)", x="y", title="Probability function X^2 con df=1") +
     scale_color_viridis_d() +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="none") +
      geom_vline(xintercept = our_chi2, color = "blue", size=1.5) +
      annotate(geom="text", x=our_g2+2.8, y=3.7, label='bold("nuestro test G^2: 1.56")', color="black", parse=TRUE, size=8) 

print(plot)
```
Si la hipótesis de independencia ( $H_{0}$ ) es cierta, nuestro $\text{test } G^{2}$ distribuye $\chi^{2}$ con $df= (I-1)(J-1)=1$
]

--

.pull-right[
.bold[p-value]

$$\mathbb{P}(\chi_{df=1}^{2} \geq \text{test } G^{2} \mid H_{0})$$
```{r}
1- pchisq(our_g2,df=1)
```

]

---
### Test Likelihood-Ratio $G^{2}$

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

our_g2 = 2*sum(ctable*log((ctable/ctable_independence))); print(our_g2)
df=1

mydata <- data_frame(x = seq(from = 0, to = 10, by =0.01), chi2 = dchisq(x,df))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
    ## Entire curve
    geom_path(aes(y=chi2,color=""), size=1.5, alpha=0.8) +
  labs(y="f(y)", x="y", title="Probability function X^2 con df=1") +
     scale_color_viridis_d() +
      theme(axis.text.y = element_text(size = 22), axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24), axis.title.x = element_text(size = 24), 
      legend.text = element_text(size = 18), legend.position="none") +
      geom_vline(xintercept = our_chi2, color = "blue", size=1.5) +
      annotate(geom="text", x=our_g2+2.8, y=3.7, label='bold("nuestro test G^2: 1.56")', color="black", parse=TRUE, size=8) 

print(plot)
```
Si la hipótesis de independencia ( $H_{0}$ ) es cierta, nuestro $\text{test } G^{2}$ distribuye $\chi^{2}$ con $df= (I-1)(J-1)=1$
]


.pull-right[
.bold[p-value]

$$\mathbb{P}(\chi_{df=1}^{2} \geq \text{test } G^{2} \mid H_{0})$$
```{r}
1- pchisq(our_g2,df=1)
```

```{r, message=FALSE}
# Versión automática
library("DescTools")
GTest(ctable, correct="none")
```

]

---
class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca





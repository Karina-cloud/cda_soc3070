---
title: "Análisis de Datos Categóricos (SOC3070)"
subtitle: "Clase #11"
author: "<br> Mauricio Bucca<br> Profesor Asistente, Sociología UC"
date: "[github.com/mebucca](https://github.com/mebucca)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default","default-fonts","gentle-r.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
---

class: inverse, center, middle

#Regresión Logística, inferencia

---
##Configuración

--

<br>

- Tenemos $y_{1}, \dots y_{n}$ son $n$ variables independientes con distribución $\text{Bernoulli}(p_{i})$


--

- $\mathbb{E}(y_{i} \mid x_{i1}, \dots,x_{ik}) = \mathbb{P}(y_{i}=1 \mid x_{i1}, \dots,x_{ik}) = p_{i}$ 

<br>
--

donde, 


$$\ln \frac{p_{i}}{1 - p_{i}} = \beta_{0} + \beta_{1} x_{i1} + \dots + \beta_{k} x_{ik}$$

---
##Configuración

<br>

- Tenemos $y_{1}, \dots y_{n}$ son $n$ variables independientes con distribución $\text{Bernoulli}(p_{i})$


- $\mathbb{E}(y_{i} \mid x_{i1}, \dots,x_{ik}) = \mathbb{P}(y_{i}=1 \mid x_{i1}, \dots,x_{ik}) = p_{i}$ 

<br>

donde, 

$$\underbrace{\ln \frac{p_{i}}{1 - p_{i}}}_{\text{Link logit}(p_{i})} = \overbrace{\beta_{0} + \beta_{1} x_{i1} + \dots + \beta_{k} x_{ik}}^{\text{Predictor lineal  } \eta_{i}}$$

<br>
--

- .bold[Inferencia]: ¿que podemos decir sobre $\hat{\beta}_{0},\hat{\beta}_{1}, \dots, \hat{\beta}_{k}$ más allá de nuestra muestra?

---
class: inverse, center, middle

## Inferencia estadística para GLMs


---
## Inferencia acerca de parámetros del modelo

- Los coeficientes de un GLM son estimados via MLE.
--
 Un ML "estimate" $\hat{\theta}$ tiene las siguientes propiedades:

--

.img-right[![MLE_prop](unbiased_consistent.png)]


.pull-left[
- Es .bold[consistente]: $\hat{\theta} \xrightarrow{p} \theta$. Es decir, en la medida que $n \to \infty$, el  estimador $\hat{\theta}$ tiende en probabilidad a $\theta$, el valor verdadero del parámetro. 
]
.pull-right[
]

--

.pull-left[
- Es .bold[insesgado]: $\mathbb{E}(\hat{\theta}) = \theta$.
]
.pull-right[
]

--

.pull-left[
- Distribuye .bold[asintónticamente normal]: $\hat{\theta} \xrightarrow{d} \mathcal{N}(\theta, \frac{\sigma_{\theta}}{\sqrt{n}})$. Es decir, no solo converge al valor verdadero, sino que converge rápidamente ( $1/\sqrt{n}$ ).
]
.pull-right[
]

--

Notar que $\frac{\sigma_{\theta}}{\sqrt{n}}$ es el "standard error" (SE) de $\theta$.

```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```

---
## Inferencia acerca de parámetros del modelo: intervalos de confianza

<br>

Por tanto, podemos construir un intervalo de confianza de la siguiente manera:

$$(1 - \alpha) \text{ CI}_{\hat{\theta}} = \hat{\theta} \pm \Phi^{-1}(\alpha/2) \cdot SE_{\hat{\theta}}$$
<br>

Ejemplo, podemos construir un intervalo al 95% de confianza está dado por:

$$95\% \text{ CI}_{\hat{\theta}} = \hat{\theta} \pm 1.96 \cdot  \frac{\sigma_{\theta}}{\sqrt{n}}$$
---
class: center, middle

## Inferencia acerca de parámetros del modelo: tests


![tests](ttest_gelman.png)
---
class: center, middle

## Delta Method


---
## Delta Method

Por series de Taylor:

$$g(X) \approx g(c) + g^{'}(c)( X - c)$$
--

En el caso de una variable aleatoria y :

$$g(X) \approx g(\theta) + g^{'}(\theta)(X - \theta) $$
<br>

\begin{align}
\mathbb{Var}[g(X)] &\approx \mathbb{Var}[ g(\theta) + g^{'}(\theta)(X - \theta)] \\ \\
&\approx \mathbb{Var}[g(\theta) + g^{'}(\theta)X   -  g^{'}(\theta)\theta] \\ \\
&\approx   [g^{'}(\theta)]^{2} \mathbb{Var}[X]   
\end{align}

---
class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca




